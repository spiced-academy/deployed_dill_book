{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2e3e72",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Ensemble Methods - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce027fc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## First Part - Necessary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d59bb6b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from auxiliary_functions import chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062505fc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Import company colors\n",
    "with open('plot_colors.json', 'r') as pc:\n",
    "    color_dict = json.load(pc)\n",
    "\n",
    "c_light, c_dark, c_blue, theme_flowchart = color_dict['color_light'], color_dict['color_dark'], color_dict['color_blue'], color_dict['theme_flowchart']\n",
    "\n",
    "# Define color map for plotting\n",
    "#color_map = ListedColormap([c_blue, c_light, c_dark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99dd53dc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate Data for plots\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a166edfb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Train model with 3 estimators and learning rate 1\n",
    "gbrt_3 = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt_3.fit(X, y)\n",
    "\n",
    "# Train model with 200 estimators and learning rate 0.1\n",
    "gbrt_200 = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gbrt_200.fit(X, y)\n",
    "\n",
    "\n",
    "def plot_ensemble(X, y, models, figsize=(15,5)):\n",
    "    \n",
    "    axis_limit = [-.5, 0.5, -0.1, 0.8]\n",
    "    \n",
    "    x1 = np.linspace(axis_limit[0], axis_limit[1], 500)\n",
    "    \n",
    "    y_pred_3 = models[0].predict(x1.reshape(-1, 1))\n",
    "    y_pred_200 = models[1].predict(x1.reshape(-1, 1))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize, sharey=True)\n",
    "    sns.set_context(\"talk\", font_scale=0.8)\n",
    "    \n",
    "    ax[0].scatter(X, y, c=c_dark, s=20)\n",
    "    ax[0].plot(x1, y_pred_3, c=c_light, linewidth=2.5, label=\"Ensemble predictions\")\n",
    "    ax[0].set(xlabel='X', ylabel='y', title='learning_rate=1.0, n_estimators=3')\n",
    "    ax[0].axis(axis_limit)\n",
    "    \n",
    "    ax[1].scatter(X, y, c=c_dark,  s=20)\n",
    "    ax[1].plot(x1, y_pred_200, c=c_light, linewidth=2.5, label=\"Ensemble predictions\")\n",
    "    ax[1].set(xlabel='X', title='learning_rate=0.1, n_estimators=200')\n",
    "    ax[0].axis(axis_limit)\n",
    "    \n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467ce7c4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "min_error = np.min(errors)\n",
    "\n",
    "\n",
    "def plot_early_stopping(X, y, model, figsize=(15,5)):\n",
    "    \n",
    "    axis_limit = [-.5, 0.5, -0.1, 0.8]\n",
    "    \n",
    "    x1 = np.linspace(axis_limit[0], axis_limit[1], 500)\n",
    "    \n",
    "    y_pred_best = model.predict(x1.reshape(-1, 1))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "    sns.set_context(\"talk\", font_scale=0.8)\n",
    "    \n",
    "    ax[0].plot(errors, c=c_blue, marker='o', markersize=5)\n",
    "    ax[0].plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "    ax[0].plot([0, 120], [min_error, min_error], \"k--\")\n",
    "    ax[0].text(gbrt_best.n_estimators_, min_error*1.2, \"Minimum\", ha=\"center\")\n",
    "    ax[0].set(xlabel='Number of trees', ylabel='Error', title='Validation Error')\n",
    "    ax[0].axis([0, 120, 0, 0.01])\n",
    "    \n",
    "    ax[1].scatter(X, y, c=c_dark,  s=20)\n",
    "    ax[1].plot(x1, y_pred_best, c=c_light, linewidth=2.5)\n",
    "    ax[1].set(xlabel='X', ylabel=\"y\", title=f\"Best model ({gbrt_best.n_estimators_} trees)\")\n",
    "    ax[1].axis(axis_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbab67",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Second Part - Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc03cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-title\">\n",
    "\n",
    "# Ensemble Methods - Part 1\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c986363",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Recap CARTs\n",
    "2. How and why do Ensembles work\n",
    "3. Different models\n",
    "4. Stacking\n",
    "4. Bagging (Random Forests, Extra Trees)\n",
    "5. Boosting\n",
    "7. Interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85a4b0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Recap CART - Classification and regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324934de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter-split\">\n",
    "    <div class=\"title-chapter\">Recap CART - Classification and regression trees</div>\n",
    "    <div class=\"image-container\">\n",
    "        <img src=\"../images/ensemble_methods/img_p2_1.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f7f2a",
   "metadata": {
    "hideOutput": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### CART \n",
    "\n",
    "Pseudocode:\n",
    "```\n",
    "fitTree (node, D, depth):\n",
    "    node.prediction = mean or class label\n",
    "    DL, DR <- split(D)\n",
    "    if not worth_splitting(depth, cost, DL, DR):\n",
    "        return node\n",
    "    else:\n",
    "        node.left  <- fitTree(node, DL, depth+1)\n",
    "        node.right <- fitTree(node, DR, depth+1)\n",
    "        return node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8835e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CART (Classification and Regression Tree)\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "\n",
    "**General:**\n",
    "\n",
    "* Finding the optimal split of the data is NP-complete.\n",
    "* CART uses a greedy procedure to compute a **locally minimal MLE** (Maximum likelihood estimation).\n",
    "* The split function chooses **best feature and best value to split on**.\n",
    "\n",
    "**Cost functions:**\n",
    "\n",
    "* Regression $\\rightarrow J(b)=\\frac{1}{n}\\sum_{i=1}^{n}{(y_{i}-\\hat{y_{i}})^2}$ (MSE)\n",
    "* Classification $\\rightarrow I_{G}(p)= 1-\\sum_{i=1}^{J}\\,p_{i}^{2}$ (Gini impurity)\n",
    "    </div>\n",
    "    <br><br><br>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/decision_tree/img_p13_1.png\">\n",
    "    </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d750235",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "NP-complete (nondeterministic polynomial-time complete) defines a problem which is easy to understand, easy to verify but difficult to solve e.g. a jigsaw puzzle.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759bc68",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: cost function for regression: J means cost\n",
    "        cost function for classification: J means number of classes, I(G) is the gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04612b7",
   "metadata": {
    "hideOutput": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CART \n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "\n",
    "**Regularisation:**\n",
    "\n",
    "* pruning to avoid overfitting\n",
    "* Grow a full tree and then prune e.g.\n",
    "    - `max_leaves`\n",
    "    - `max_depth`\n",
    "    - `min_sample_size`\n",
    "    </div>\n",
    "    <br><br><br>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/decision_tree/img_p13_1.png\">\n",
    "    </div>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab318d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### CART - pros and cons\n",
    "\n",
    "<br>\n",
    "\n",
    "<table class=\"confusion_matrix\" border=\"1\" width=\"1800\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th><b>&nbsp;&nbsp;PROS&nbsp;&nbsp;</b></th>\n",
    "      <th><b>&nbsp;&nbsp;CONS&nbsp;&nbsp;</b></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f38b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CART - pros and cons\n",
    "\n",
    "<br>\n",
    "\n",
    "<table class=\"confusion_matrix\" border=\"1\" width=\"1800\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th><b>PROS</b></th>\n",
    "      <th><b>CONS</b></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">white box model - easy to interpret</td>\n",
    "      <td class=\"smaller-font-size\">not very accurate ...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">can handle mixed data, discrete and continuous</td>\n",
    "      <td class=\"smaller-font-size\">greedy nature of constructing</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">insensitive to transformations of data... split points based on ranking  </td>\n",
    "      <td class=\"smaller-font-size\">trees are unstable, small changes to the input can lead to large changes in tree structure</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">relative robust to outliers</td>\n",
    "      <td class=\"smaller-font-size\">tend to overfit (aka high variance estimators)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05d471",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trees being unstable\n",
    "\n",
    "**Q:** Should you trust and follow a single CART?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca98095",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">       \n",
    "        \n",
    "* Changes in your trees can lead to different structure and maybe different decisions\n",
    "* As in investment, it is not advisable to lay all eggs in one basket\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"images_30\">   \n",
    "        <img src=\"../images/ensemble_methods/img_p7_1.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5a2ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trees being greedy and overfitting\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "\n",
    "How to find out what to order at the restaurant?\n",
    "* **Try everything!** <br>... and then order what you liked best.\n",
    "* **Ask your friends to try something**, <br>find out which they preferred more and try those maybe?\n",
    "\n",
    "**Q:** Which do you find more feasible?\n",
    "    </div>\n",
    "    <br><br><br>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/ensemble_methods/eating_cloud.png\">\n",
    "    </div>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3cb57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/ensemble_methods/img_p9_1.png\" position=\"center\" size=\"cover\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53a652",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Who wants to be a millionaire allows contestants to get help from a friend and help from the audience. Over the run of the show, which help type do you think performed better? by how much? The majority of the audience picked the right answer 91 percent of the time, while individual friends only did so 65 percent of the time. This is an example of “crowd wisdom”, it’s an idea, summarized in the 2004 book by James Surowiecki by the same name, which states that the aggregate information in a group often leads to a better decision than any single member of the group. Image licence: unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4b24d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/ensemble_methods/img_p10_1.png\" position=\"center\" width=800>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13835a0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: What about Wikipedia? I think we can all agree it is a fantastic CROWD SOURCED project for freedom of information, accessibility and relevance. But is it also “crowd wisdom”? Does it create a more reliable knowledge base than sources that aren’t crowd sourced, such as encyclopedias, text books and news sources. If you are interested, you can read a very LONG and very interesting wikipedia article about its own reliability (if you don’t mind the paradox). for crowd wisdom we need the following conditions: image licence: free to use https://commons.wikimedia.org/wiki/File:WikipediaLogo-TheOfficiaFour.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba63fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wisdom of Crowds Theory\n",
    "\n",
    "* Diversity of opinions\n",
    "* Independence of opinions\n",
    "* Decentralisation\n",
    "* Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf596f04",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Let’s see what are the requirements or assumptions of the Crowd Wisdom theory: 1. Each person has private information. 2. People aren’t influenced by those around them. 3. People are specialised and can draw on local knowledge. 4. Some mechanism exists to turn private judgements into a collective decision. Do you think they hold for both WWtBaM and Wikipedia? <CLICK>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39784db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Who Wants to Be a Millionaire 💸  //  📖 Wikipedia\n",
    "\n",
    "<br>\n",
    "\n",
    "<table class=\"confusion_matrix\" >\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th><b>Who Wants to Be a Millionaire 💸</b></th>\n",
    "      <th><b>Wikipedia 📖</b></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td colspan=2 class=\"cell-class\">Each person has private information.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">The audience members should have opinions of their own.</td>\n",
    "      <td class=\"smaller-font-size\">Wikipedians have opinions of their own.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td colspan=2 class=\"cell-class\">Independence of opinions.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">There might be some small local effects.</td>\n",
    "      <td class=\"smaller-font-size\">Wikipedians often don’t know each other.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td colspan=2 class=\"cell-class\">Decentralisation</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">Trivia knowledge isn’t specialised. People can make educated guesses.</td>\n",
    "      <td class=\"smaller-font-size\">Wikipedia is decentralized by definition. Single-author pages require review.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td colspan=2 class=\"cell-class\">Aggregation</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"smaller-font-size\">People chose the most popular vote: max()</td>\n",
    "      <td class=\"smaller-font-size\">Talk pages to discuss and mediate disagreements. Decisions by consensus or executive decision. Loss of independence or lack of aggregation.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022ef5e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: For Wikipedia, probably there are some edge cases where there aren’t enough editors for highly specialised knowledge. But from my experience in academia, there isn’t a topic you can’t find two opposing opinions on ;) in WWtBaM (I’m not sure how much time the audience gets to decide. They might speak to their friend or neighbour). Is it true for Wiki, certainly in some cases the editors don’t know each other. Maybe in highly specialised articles there isn’t independance. People are specialised and can draw on local knowledge. This certainly seems true for both. Trivia knowledge isn’t so specialised by definition, so at least some audience members could make educated guesses, if they don’t know the precise answer.Wikipedia, is decentralised by definition to my best understanding. Pages that are single authored are often marked as requiring review. Some mechanism exists to turn private judgements into a collective decision. As we already saw, WWtBaM counts the number of votes per possible answer.But Wikipedia doesn’t aggregate information. When there is diversity of opinions, as required by the first assumption, the editors use the talk pages to discuss, mediate and come to an agreement. This is a way to reach consensus, but it’s not aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7947f38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reducing variance by aggregation\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/ensemble_methods/img_p14_1.png\" position=\"center\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33589806",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: I hope you’re starting to find the Wisdom of Crowds as interesting as I do. This is also <CLICK> the underlying theory of reducing the variance by aggregation, which you might also known as the philosophical idea behind a ML algorithm called Random Forest. In which we partition the data to small subsets, make an estimation based on each small subset, and then aggregate the result to get a better estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1174a25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble learning... asking for more opinions and making a judgment call\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "\n",
    "A collection of models working together on same dataset\n",
    "\n",
    "**Mechanism:**\n",
    "* majority voting\n",
    "\n",
    "**There are different ways to do it:**\n",
    "* **<span class=\"color-brand\">bagging</span>** (same model on different parts of the data)\n",
    "* **<span class=\"color-brand\">boosting</span>** (seq. adjusts for importance of observations)\n",
    "* **<span class=\"color-brand\">stacking / blending</span>** (training in parallel and combining)\n",
    "\n",
    "**Advantages:**\n",
    "* less sensitive to overfitting\n",
    "* better model performance\n",
    "    </div>\n",
    "    <br><br><br>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/ensemble_methods/more_opinions.png\">\n",
    "    </div>\n",
    "</div> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757300b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Majority Voting - how it works\n",
    "\n",
    "For **each observation** you have **different predictions** from **different models**\n",
    "\n",
    "You need one prediction... so you aggregate\n",
    "\n",
    "Aggregating the different models can be done in many ways e.g. averages, mode, median\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd575d2",
   "metadata": {
    "cell_style": "split",
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": true,
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/JSV7aW5pdDogeyd0aGVtZSc6J2Jhc2UnLCAndGhlbWVWYXJpYWJsZXMnOiB7ICdiYWNrZ3JvdW5kJzonI0ZGRkZGRicsICdmb250U2l6ZSc6ICcxNnB4JywgJ2ZvbnRGYW1pbHknOiAndHJlYnVjaGV0IG1zJywgJ3ByaW1hcnlDb2xvcic6ICcjRkZBNDg4JywgJ3ByaW1hcnlCb3JkZXJDb2xvcic6ICcjRkY0QTExJywgJ3NlY29uZGFyeUNvbG9yJzogJyNDM0M0QzgnLCAndGVydGlhcnlDb2xvcic6ICcjQjRERkZGJywgJ3RlcnRpYXJ5Qm9yZGVyQ29sb3InOiAnIzMzQTVGRicsICdsaW5lQ29sb3InOiAnIzI1MjYyOSd9fX0lJQpmbG93Y2hhcnQgVEQKICAgIEEoVHJhaW5pbmcgU2V0KSAtLT4gQihNb2RlbCAxIAogICAgZmE6ZmEtY29ncykgLS1OZXcgSW5zdGFuY2UtLT4gRigoIDEgKSkgLS0+IEp7Vm90aW5nfSAtLT4gSygoMSkpCiAgICBBIC0tPiBDKE1vZGVsIDIKICAgIGZhOmZhLWNvZ3MpIC0tTmV3IEluc3RhbmNlLS0+IEcoKCAwICkpIC0tPiBKCiAgICBBIC0tPiBEKE1vZGVsIDMgCiAgICBmYTpmYS1jb2dzKSAtLU5ldyBJbnN0YW5jZS0tPiBIKCggMSApKSAtLT4gSgogICAgQSAtLT4gRShNb2RlbCA0CiAgICBmYTpmYS1jb2dzKSAtLU5ldyBJbnN0YW5jZS0tPiBJKCggMSApKSAtLT4gSgogICAgc3ViZ3JhcGggIkVuc2VtYmxlIgogICAgICAgIEIKICAgICAgICBDCiAgICAgICAgRAogICAgICAgIEUKICAgIGVuZAo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart(\"\"\"\n",
    "flowchart TD\n",
    "    A(Training Set) --> B(Model 1 \n",
    "    fa:fa-cogs) --New Instance--> F(( 1 )) --> J{Voting} --> K((1))\n",
    "    A --> C(Model 2\n",
    "    fa:fa-cogs) --New Instance--> G(( 0 )) --> J\n",
    "    A --> D(Model 3 \n",
    "    fa:fa-cogs) --New Instance--> H(( 1 )) --> J\n",
    "    A --> E(Model 4\n",
    "    fa:fa-cogs) --New Instance--> I(( 1 )) --> J\n",
    "    subgraph \"Ensemble\"\n",
    "        B\n",
    "        C\n",
    "        D\n",
    "        E\n",
    "    end\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cf295",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "Methods based on the values of the probabilities need to return calibrated probabilities.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf54911",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting - how it works\n",
    "\n",
    "For each observation you have different predictions. How do you aggregate this to get one value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0217f9b",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "**<center>Hard Voting &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;</center>**\n",
    "<center>takes the majority class  &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f69a7a",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "**<center>Soft Voting &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;</center>**\n",
    "<center>average of summed probability vectors &nbsp;  &nbsp;  &nbsp;  &nbsp; &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6f029a",
   "metadata": {
    "cell_style": "split",
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": false,
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/JSV7aW5pdDogeyd0aGVtZSc6J2Jhc2UnLCAndGhlbWVWYXJpYWJsZXMnOiB7ICdiYWNrZ3JvdW5kJzonI0ZGRkZGRicsICdmb250U2l6ZSc6ICcxNnB4JywgJ2ZvbnRGYW1pbHknOiAndHJlYnVjaGV0IG1zJywgJ3ByaW1hcnlDb2xvcic6ICcjRkZBNDg4JywgJ3ByaW1hcnlCb3JkZXJDb2xvcic6ICcjRkY0QTExJywgJ3NlY29uZGFyeUNvbG9yJzogJyNDM0M0QzgnLCAndGVydGlhcnlDb2xvcic6ICcjQjRERkZGJywgJ3RlcnRpYXJ5Qm9yZGVyQ29sb3InOiAnIzMzQTVGRicsICdsaW5lQ29sb3InOiAnIzI1MjYyOSd9fX0lJQpmbG93Y2hhcnQgVEQKICAgIEEoVHJhaW5pbmcgU2V0KSAtLT4gQihNb2RlbCAxIAogICAgZmE6ZmEtY29ncykgLS1OZXcgSW5zdGFuY2UtLT4gRigoIDAgKSkgLS0+IEp7bW9kZQogICAgMH0gLS0+IEsoKDApKQogICAgQSAtLT4gQyhNb2RlbCAyCiAgICBmYTpmYS1jb2dzKSAtLU5ldyBJbnN0YW5jZS0tPiBHKCggMSApKSAtLT4gSgogICAgQSAtLT4gRChNb2RlbCAzIAogICAgZmE6ZmEtY29ncykgLS1OZXcgSW5zdGFuY2UtLT4gSCgoIDAgKSkgLS0+IEoKICAgIEEgLS0+IEUoTW9kZWwgNAogICAgZmE6ZmEtY29ncykgLS1OZXcgSW5zdGFuY2UtLT4gSSgoIDAgKSkgLS0+IEoKICAgIHN1YmdyYXBoICJFbnNlbWJsZSIKICAgICAgICBCCiAgICAgICAgQwogICAgICAgIEQKICAgICAgICBFCiAgICBlbmQK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart(\"\"\"\n",
    "flowchart TD\n",
    "    A(Training Set) --> B(Model 1 \n",
    "    fa:fa-cogs) --New Instance--> F(( 0 )) --> J{mode\n",
    "    0} --> K((0))\n",
    "    A --> C(Model 2\n",
    "    fa:fa-cogs) --New Instance--> G(( 1 )) --> J\n",
    "    A --> D(Model 3 \n",
    "    fa:fa-cogs) --New Instance--> H(( 0 )) --> J\n",
    "    A --> E(Model 4\n",
    "    fa:fa-cogs) --New Instance--> I(( 0 )) --> J\n",
    "    subgraph \"Ensemble\"\n",
    "        B\n",
    "        C\n",
    "        D\n",
    "        E\n",
    "    end\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b8b56f",
   "metadata": {
    "cell_style": "split",
    "hideCode": true,
    "hidePrompt": true,
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/JSV7aW5pdDogeyd0aGVtZSc6J2Jhc2UnLCAndGhlbWVWYXJpYWJsZXMnOiB7ICdiYWNrZ3JvdW5kJzonI0ZGRkZGRicsICdmb250U2l6ZSc6ICcxNnB4JywgJ2ZvbnRGYW1pbHknOiAndHJlYnVjaGV0IG1zJywgJ3ByaW1hcnlDb2xvcic6ICcjRkZBNDg4JywgJ3ByaW1hcnlCb3JkZXJDb2xvcic6ICcjRkY0QTExJywgJ3NlY29uZGFyeUNvbG9yJzogJyNDM0M0QzgnLCAndGVydGlhcnlDb2xvcic6ICcjQjRERkZGJywgJ3RlcnRpYXJ5Qm9yZGVyQ29sb3InOiAnIzMzQTVGRicsICdsaW5lQ29sb3InOiAnIzI1MjYyOSd9fX0lJQpmbG93Y2hhcnQgVEQKICAgIEEoVHJhaW5pbmcgU2V0KSAtLT4gQihNb2RlbCAxIAogICAgZmE6ZmEtY29ncykgLS1OZXcgSW5zdGFuY2UtLT4gRigoIDAuNDEgKSkgLS0+IEp7bWVhbgogICAgMC41Nn0gLS0+IEsoKDEpKQogICAgQSAtLT4gQyhNb2RlbCAyCiAgICBmYTpmYS1jb2dzKSAtLU5ldyBJbnN0YW5jZS0tPiBHKCggMC45OSApKSAtLT4gSgogICAgQSAtLT4gRChNb2RlbCAzIAogICAgZmE6ZmEtY29ncykgLS1OZXcgSW5zdGFuY2UtLT4gSCgoIDAuMzUgKSkgLS0+IEoKICAgIEEgLS0+IEUoTW9kZWwgNAogICAgZmE6ZmEtY29ncykgLS1OZXcgSW5zdGFuY2UtLT4gSSgoIDAuNDggKSkgLS0+IEoKICAgIHN1YmdyYXBoICJFbnNlbWJsZSIKICAgICAgICBCCiAgICAgICAgQwogICAgICAgIEQKICAgICAgICBFCiAgICBlbmQK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart(\"\"\"\n",
    "flowchart TD\n",
    "    A(Training Set) --> B(Model 1 \n",
    "    fa:fa-cogs) --New Instance--> F(( 0.41 )) --> J{mean\n",
    "    0.56} --> K((1))\n",
    "    A --> C(Model 2\n",
    "    fa:fa-cogs) --New Instance--> G(( 0.99 )) --> J\n",
    "    A --> D(Model 3 \n",
    "    fa:fa-cogs) --New Instance--> H(( 0.35 )) --> J\n",
    "    A --> E(Model 4\n",
    "    fa:fa-cogs) --New Instance--> I(( 0.48 )) --> J\n",
    "    subgraph \"Ensemble\"\n",
    "        B\n",
    "        C\n",
    "        D\n",
    "        E\n",
    "    end\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2248c1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "Soft voting tends to outperform hard voting.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b1b2a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## V. Classifier calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f29112",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter\">\n",
    "\n",
    "## V. Classifier calibration\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618c9f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Classifier calibration\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "        \n",
    "Fitting a regressor (called a calibrator) that maps the output of the classifier (decision_function or predict_proba) to a calibrated probability in [0, 1]\n",
    "\n",
    "[sklearn - calibrated probabilities](https://scikit-learn.org/stable/modules/calibration.html)\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/ensemble_methods/img_p56_3.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bbbfe6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Calibrating a classifier consists of fitting a regressor (called a calibrator) that maps the output of the classifier (as given by decision_function or predict_proba) to a calibrated probability in [0, 1]. Denoting the output of the classifier for a given sample by fi , the calibrator tries to predict p(yi=1|fi) . The samples that are used to fit the calibrator should not be the same samples used to fit the classifier, as this would introduce bias. This is because performance of the classifier on its training data would be better than for novel data. Using the classifier output of training data to fit the calibrator would thus result in a biased calibrator that maps to probabilities closer to 0 and 1 than it should."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e22266",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting - why it works\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "\n",
    "**Intuition:** at observation level not all classifiers will be wrong in the same way at the same time\n",
    "\n",
    "**Prerequisites** for this to work:\n",
    "* models need to be better than random, i.e. **weak learners**\n",
    "* you want the models to be different and that get translated in statistics to non-correlated or **sufficiently independent** or .. diverse\n",
    "* you need a **sufficient amount** of weak learners \n",
    "    </div>\n",
    "    <br><br><br>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/ensemble_methods/multible_learners.png\">\n",
    "    </div>\n",
    "</div> \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "A weak learner model has an average true positive rate slightly better than 50% and an average false positive rate slightly less than 50%.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a560d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting - why it works\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "\n",
    "**What does it mean that the models are independent/ different?**\n",
    "\n",
    "If two weak learners would be correlated then a observation is likely to have same prediction by both weak learners.\n",
    "\n",
    "**Why do you think it is important?**\n",
    "\n",
    "Probability of an observation x to be misclassified by the ensemble learner is basically equivalent to it being misclassified by a majority of the weak learners.\n",
    "    </div>\n",
    "    <br>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/ensemble_methods/weak_learners.png\">\n",
    "        **<span class=\"color-brand\">A weak learner is 51% right and 49% wrong</span>**\n",
    "    </div>\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "This can be proven by math. Feel free :D to try.. or <br>\n",
    "run 1000 coin toss experiments and aggregate.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b21be8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting - why it works\n",
    "\n",
    "$X_i$ are independent identically distributed:\n",
    "\n",
    "$\\text{Var} (X_{i})=\\sigma^{2}$\n",
    "\n",
    "$\\text{Var} (\\bar{X})=\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right) = \\frac{\\sigma^2}{n}$, according to the central limit theorem.\n",
    "\n",
    "\n",
    "\n",
    "Drop the independence assumption $X_i$ are correlated by $\\rho$:\n",
    "\n",
    "$ \\text{Var}(\\bar{X})=\\rho\\sigma^{2}+(1-\\rho)\\frac{\\sigma^{2}}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2a0fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of weak learners\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "\n",
    "$$m=1000, \\ p=0.51$$\n",
    "        \n",
    "$$p(x<k) = cdf(k=500) = 27.4\\%$$\n",
    "\n",
    "        \n",
    "Chances to be wrong with m = 1000 weak learners.\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        Cumulative distribution function (cdf)\n",
    "        <img src=\"../images/ensemble_methods/img_p21_1.png\" width=400>\n",
    "    </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f71c8b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The question “answered” here is: If we have an unfair coin, that is a coin that shows HEADS 51% (p=.51) and TAILS only 49% of the time. Experiment: Let's flip the coin 1000 times (n=1000) and count how often we got HEADS (K) We would expect for each run of the experiment to get 510 HEADS (=expected value =n*p ) and 490 TAILS (sometimes a little more, sometimes a little less) Now, let's conduct the experiment a million times and remember how often we got heads(K). If we make a histogram for K (~p(x=k)), we would expect to see a bell-curve. Most of the time we get 510, almost as often 509 or 511, almost as often 508 or 512… with decreasing occurrences left and right of the expected 510. If we take the histogram start from the left and successively add the values we encounter, we get to the cpf, i.e. the cumulative probability function which shows us the probability of getting “less than k” observations. (Shown on this slide) In our example, the chance of getting less than 1001 Heads is? p=1. We cannot get more than 1000 Heads in 1000 throws. the chance of getting less than 540 heads is (according to the plot) ~98% (-> or if you look at it from the other way: the chances of getting more than 540 heads in our 1000 throws is already only about 2%) the chance of getting less than 510 heads is (according to the plot) =50% (because this is exactly the expected value!) To bring it back to the learners: Each Weak learner is basically a coin flip with a slightly unfair coin: if each of the 1000 weak learners has an accuracy of 51% (and they are all completely independent from each other). Making a prediction now corresponds to the experiment from above: we “flip” each weak learner and the result. We would expect (given the accuracy) to get for each experiment run (about) 510 right and 490 wrong answers. But – it could also be 505 (right)vs 495(wrong)… or 496(right) vs 504(wrong). If we apply majority voting, the first two (510 / 505 right) would make us take the correct prediction, whereas the last one (496 right) would make us take the wrong prediction. The magic point here is: how often are less than 500 of our models correct (or in a formula p(x<k=500)). From the shown graph we can see that this is 27,4%. SO If we select the majority answer from 1000 weak learners with an accuracy of just 51% -> we get an ensemble accuracy of 72,6% (= 100-27.4)! If we do the same thing, but we make our learners slightly better, i.e. they have an accuracy of 55% percent vs 51%, we get the cdf from the next slide. Then the chance of less than 500 of our models being correct (or in a formula p(x<k=500) is down to 0,08% -> so we get an ensemble accuracy of 99,92%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3a375",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of weak learners\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "\n",
    "$$m=1000, \\ p=0.55$$\n",
    "\n",
    "$$p(x<k) = cdf(k=500) = 0.08\\% $$\n",
    "        \n",
    "Chances to be wrong with m = 1000 weak learners.\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        Cumulative distribution function (cdf)\n",
    "        <img src=\"../images/ensemble_methods/img_p22_1.png\" width=400>\n",
    "    </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a0b6a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The question “answered” here is: If we have an unfair coin, that is a coin that shows HEADS 51% (p=.51) and TAILS only 49% of the time. Experiment: Let's flip the coin 1000 times (n=1000) and count how often we got HEADS (K) We would expect for each run of the experiment to get 510 HEADS (=expected value =n*p ) and 490 TAILS (sometimes a little more, sometimes a little less) Now, let's conduct the experiment a million times and remember how often we got heads(K). If we make a histogram for K (~p(x=k)), we would expect to see a bell-curve. Most of the time we get 510, almost as often 509 or 511, almost as often 508 or 512… with decreasing occurrences left and right of the expected 510. If we take the histogram start from the left and successively add the values we encounter, we get to the cpf, i.e. the cumulative probability function which shows us the probability of getting “less than k” observations. (Shown on this slide) In our example, the chance of getting less than 1001 Heads is? p=1. We cannot get more than 1000 Heads in 1000 throws. the chance of getting less than 540 heads is (according to the plot) ~98% (-> or if you look at it from the other way: the chances of getting more than 540 heads in our 1000 throws is already only about 2%) the chance of getting less than 510 heads is (according to the plot) =50% (because this is exactly the expected value!) To bring it back to the learners: Each Weak learner is basically a coin flip with a slightly unfair coin: if each of the 1000 weak learners has an accuracy of 51% (and they are all completely independent from each other). Making a prediction now corresponds to the experiment from above: we “flip” each weak learner and the result. We would expect (given the accuracy) to get for each experiment run (about) 510 right and 490 wrong answers. But – it could also be 505 (right)vs 495(wrong)… or 496(right) vs 504(wrong). If we apply majority voting, the first two (510 / 505 right) would make us take the correct prediction, whereas the last one (496 right) would make us take the wrong prediction. The magic point here is: how often are less than 500 of our models correct (or in a formula p(x<k=500)). From the shown graph we can see that this is 27,4%. SO If we select the majority answer from 1000 weak learners with an accuracy of just 51% -> we get an ensemble accuracy of 72,6% (= 100-27.4)! If we do the same thing, but we make our learners slightly better, i.e. they have an accuracy of 55% percent vs 51%, we get the cdf from the next slide. Then the chance of less than 500 of our models being correct (or in a formula p(x<k=500) is down to 0,08% -> so we get an ensemble accuracy of 99,92%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac094e1b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## How does my model look as an ensemble model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f572f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter\">\n",
    "\n",
    "## How does my model look as an ensemble model?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10269a20",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Adaptive basis function models\n",
    "\n",
    "$$ f(x)=b_{0}~\\ + \\sum b_{m}\\phi_{m}(x)$$\n",
    "\n",
    "This framework covers many models: decision trees, neural networks, random forest, boosting trees and so on... basically we have m parametric basis functions.\n",
    "\n",
    "**How does the formula look like for a decision tree?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac220ece",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Basis functions (called derived features in machine learning) are building blocks for creating more complex functions. In other words, they are a set of k standard functions, combined to estimate another function—one which is difficult or impossible to model exactly. For example, individuals powers of x— the basis functions 1, x, x2, x3…— can be strung together to form a polynomial function. The set of basis functions used to create the more complex function is called a basis set. Fitting the data with basis functions is possible, but requires choosing appropriate parameters, instead we could use Adaptive basis functions that learn the parameters by themselves. We gain performance of the model, but loose interpretability. https://www.oranlooney.com/post/adaptive-basis-functions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b822ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter-split\">\n",
    "    <div class=\"title-chapter\">BREAK</div>\n",
    "    <div class=\"image-container\">\n",
    "        <img src=\"../images/general/break.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03531f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## How do I produce different models with the same data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bccce9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter\">\n",
    "\n",
    "## How do I produce different models with the same data?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89dffc8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## I. Different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958abc1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter\">\n",
    "\n",
    "## I. Different models\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f2fa8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using different models\n",
    "\n",
    "Really different models, not necessarily weak models. So you could use 3 or more of different types:\n",
    "\n",
    "* decision trees\n",
    "* random forest\n",
    "* neural networks\n",
    "* SVMs\n",
    "* NBs\n",
    "* KNN\n",
    "* Ridge Classifier\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e496fb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## II. Stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278c99d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter\">\n",
    "\n",
    "## II. Stacking\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb830e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a model to aggregate\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "\n",
    "* **train** weak learners in **parallel**\n",
    "* combine by choosing the best for each observation\n",
    "* this will obviously overfit\n",
    "* using cross-validation to avoid overfitting\n",
    "        \n",
    "Applications: netflix competition winners in 2009 \n",
    "\n",
    "\n",
    "Blending (holdout)  → Stacking (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c4a5cc",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/JSV7aW5pdDogeyd0aGVtZSc6J2Jhc2UnLCAndGhlbWVWYXJpYWJsZXMnOiB7ICdiYWNrZ3JvdW5kJzonI0ZGRkZGRicsICdmb250U2l6ZSc6ICcxNnB4JywgJ2ZvbnRGYW1pbHknOiAndHJlYnVjaGV0IG1zJywgJ3ByaW1hcnlDb2xvcic6ICcjRkZBNDg4JywgJ3ByaW1hcnlCb3JkZXJDb2xvcic6ICcjRkY0QTExJywgJ3NlY29uZGFyeUNvbG9yJzogJyNDM0M0QzgnLCAndGVydGlhcnlDb2xvcic6ICcjQjRERkZGJywgJ3RlcnRpYXJ5Qm9yZGVyQ29sb3InOiAnIzMzQTVGRicsICdsaW5lQ29sb3InOiAnIzI1MjYyOSd9fX0lJQpmbG93Y2hhcnQgVEQKICAgIEEoTmV3IGluc3RhbmNlKSAtLT4gQihNb2RlbCAxIAogICAgZmE6ZmEtY29ncykgLS0+IEYoKCAzLjEgKSkgLS0+IEp7YmxlbmRpbmcKICAgIH0gLS0+IEsoKDMuMCkpCiAgICBBIC0tPiBDKE1vZGVsIDIKICAgIGZhOmZhLWNvZ3MpIC0tPiBHKCggMi43ICkpIC0tPiBKCiAgICBBIC0tPiBEKE1vZGVsIDMgCiAgICBmYTpmYS1jb2dzKSAtLT4gSCgoIDIuOSApKSAtLT4gSgoKICAgIHN1YmdyYXBoICJQcmVkaWN0IgogICAgICAgIEIKICAgICAgICBDCiAgICAgICAgRAogICAgZW5kCg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart(\"\"\"\n",
    "flowchart TD\n",
    "    A(New instance) --> B(Model 1 \n",
    "    fa:fa-cogs) --> F(( 3.1 )) --> J{blending\n",
    "    } --> K((3.0))\n",
    "    A --> C(Model 2\n",
    "    fa:fa-cogs) --> G(( 2.7 )) --> J\n",
    "    A --> D(Model 3 \n",
    "    fa:fa-cogs) --> H(( 2.9 )) --> J\n",
    "\n",
    "    subgraph \"Predict\"\n",
    "        B\n",
    "        C\n",
    "        D\n",
    "    end\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ad6e3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Der Trick funktioniert über Datenset-Splitting. Der Blender muss auf anderen Daten trainiert werden als die Predictors. Features sind die Outputs der Predictors und Labels die Original label. Level 0 Model (Basemodel) Level 1 model (Meta-Mode) -> trained on holdout data Blending: Stacking-type ensemble where the meta-model is trained on predictions made on a holdout dataset. Stacking: Stacking-type ensemble where the meta-model is trained on out-of-fold predictions made during k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f2b79",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## III. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64826aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-chapter\">\n",
    "\n",
    "## III. Bagging\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a495e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Bagging\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "\n",
    "<span class=\"color-brand\" style=\"font-weight:bold;\">B</span>ootsprap <span class=\"color-brand\" style=\"font-weight:bold;\">agg</span>regat<span class=\"color-brand\" style=\"font-weight:bold;\">ing</span>\n",
    "\n",
    "$$ f(x)= \\frac{1}{m}\\sum f_{m}(x)$$  \n",
    "        \n",
    "Training $m$ **different trees** on **different subsets** of data chosen randomly with replacement.\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/ensemble_methods/img_p28_3.png\">\n",
    "    </div>\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "Running same learning algorithms on different subsets can result in still highly correlated predictors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0257d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging\n",
    "\n",
    "<span class=\"color-brand\" style=\"font-weight:bold;\">B</span>ootsprap <span class=\"color-brand\" style=\"font-weight:bold;\">agg</span>regat<span class=\"color-brand\" style=\"font-weight:bold;\">ing</span>\n",
    "\n",
    "$$ f(x)= \\frac{1}{m}\\sum f_{m}(x)$$  \n",
    "        \n",
    "Training $m$ **different trees** on **different subsets** of data chosen randomly with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2321e092",
   "metadata": {
    "cell_style": "split",
    "hideCode": true,
    "hidePrompt": true,
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/JSV7aW5pdDogeyd0aGVtZSc6J2Jhc2UnLCAndGhlbWVWYXJpYWJsZXMnOiB7ICdiYWNrZ3JvdW5kJzonI0ZGRkZGRicsICdmb250U2l6ZSc6ICcxNnB4JywgJ2ZvbnRGYW1pbHknOiAndHJlYnVjaGV0IG1zJywgJ3ByaW1hcnlDb2xvcic6ICcjRkZBNDg4JywgJ3ByaW1hcnlCb3JkZXJDb2xvcic6ICcjRkY0QTExJywgJ3NlY29uZGFyeUNvbG9yJzogJyNDM0M0QzgnLCAndGVydGlhcnlDb2xvcic6ICcjQjRERkZGJywgJ3RlcnRpYXJ5Qm9yZGVyQ29sb3InOiAnIzMzQTVGRicsICdsaW5lQ29sb3InOiAnIzI1MjYyOSd9fX0lJQpmbG93Y2hhcnQgVEQKICAgIEEoVHJhaW5pbmcgRGF0YQogICAgb2JzZXJ2YXRpb24gMQogICAgb2JzZXJ2YXRpb24gMgogICAgb2JzZXJ2YXRpb24gMwogICAgb2JzZXJ2YXRpb24gNAogICAgb2JzZXJ2YXRpb24gNQogICAgKSAtLT4gQihCb290c3RyYXBwZWQgRGF0YSAxCiAgICBvYnNlcnZhdGlvbiAxCiAgICBvYnNlcnZhdGlvbiAyCiAgICBvYnNlcnZhdGlvbiAyCiAgICBvYnNlcnZhdGlvbiAzCiAgICBvYnNlcnZhdGlvbiA1KSAtLT4gRihNb2RlbCAxIAogICAgZmE6ZmEtY29ncykgLS0+IEooTWFqb3JpdHkgVm90aW5nKQogICAgQSAtLT4gQyhCb290c3RyYXBwZWQgRGF0YSAyCiAgICBvYnNlcnZhdGlvbiAyCiAgICBvYnNlcnZhdGlvbiAzCiAgICBvYnNlcnZhdGlvbiA0CiAgICBvYnNlcnZhdGlvbiA0CiAgICBvYnNlcnZhdGlvbiA1KSAtLT4gRyhNb2RlbCAyIAogICAgZmE6ZmEtY29ncykgLS0+IEoKICAgIEEgLS0+IEQoQm9vdHN0cmFwcGVkIERhdGEgMwogICAgb2JzZXJ2YXRpb24gMgogICAgb2JzZXJ2YXRpb24gMgogICAgb2JzZXJ2YXRpb24gNQogICAgb2JzZXJ2YXRpb24gNQogICAgb2JzZXJ2YXRpb24gNSkgLS0+IEgoTW9kZWwgMwogICAgZmE6ZmEtY29ncykgLS0+IEoKICAgIEEgLS0+IEUoQm9vdHN0cmFwcGVkIERhdGEgNAogICAgb2JzZXJ2YXRpb24gMQogICAgb2JzZXJ2YXRpb24gMQogICAgb2JzZXJ2YXRpb24gMgogICAgb2JzZXJ2YXRpb24gNAogICAgb2JzZXJ2YXRpb24gNCktLT4gSShNb2RlbCA0IAogICAgZmE6ZmEtY29ncykgLS0+IEoKICAgIHN1YmdyYXBoICJCYWdnaW5nIgogICAgICAgIEIKICAgICAgICBDCiAgICAgICAgRAogICAgICAgIEUKICAgICAgICBGCiAgICAgICAgRwogICAgICAgIEgKICAgICAgICBJCiAgICAgICAgc3ViZ3JhcGggIkJvb3RzdHJhcHBpbmciCiAgICAgICAgICAgIEIKICAgICAgICAgICAgQwogICAgICAgICAgICBECiAgICAgICAgICAgIEUKICAgICAgICBlbmQKICAgICAgICBzdWJncmFwaCAiQWdncmVnYXRpbmciCiAgICAgICAgICAgIEYKICAgICAgICAgICAgRwogICAgICAgICAgICBICiAgICAgICAgICAgIEkKICAgICAgICBlbmQKICAgIGVuZAoK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart(\"\"\"\n",
    "flowchart TD\n",
    "    A(Training Data\n",
    "    observation 1\n",
    "    observation 2\n",
    "    observation 3\n",
    "    observation 4\n",
    "    observation 5\n",
    "    ) --> B(Bootstrapped Data 1\n",
    "    observation 1\n",
    "    observation 2\n",
    "    observation 2\n",
    "    observation 3\n",
    "    observation 5) --> F(Model 1 \n",
    "    fa:fa-cogs) --> J(Majority Voting)\n",
    "    A --> C(Bootstrapped Data 2\n",
    "    observation 2\n",
    "    observation 3\n",
    "    observation 4\n",
    "    observation 4\n",
    "    observation 5) --> G(Model 2 \n",
    "    fa:fa-cogs) --> J\n",
    "    A --> D(Bootstrapped Data 3\n",
    "    observation 2\n",
    "    observation 2\n",
    "    observation 5\n",
    "    observation 5\n",
    "    observation 5) --> H(Model 3\n",
    "    fa:fa-cogs) --> J\n",
    "    A --> E(Bootstrapped Data 4\n",
    "    observation 1\n",
    "    observation 1\n",
    "    observation 2\n",
    "    observation 4\n",
    "    observation 4)--> I(Model 4 \n",
    "    fa:fa-cogs) --> J\n",
    "    subgraph \"Bagging\"\n",
    "        B\n",
    "        C\n",
    "        D\n",
    "        E\n",
    "        F\n",
    "        G\n",
    "        H\n",
    "        I\n",
    "        subgraph \"Bootstrapping\"\n",
    "            B\n",
    "            C\n",
    "            D\n",
    "            E\n",
    "        end\n",
    "        subgraph \"Aggregating\"\n",
    "            F\n",
    "            G\n",
    "            H\n",
    "            I\n",
    "        end\n",
    "    end\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c801af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "Running same learning algorithms on different subsets can result in still highly correlated predictors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22616422",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Bootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples. In our context the dataset is split into a number of subsets similar to how bootstrapping is traditionally performed. Sampling with replacement ensures each bootstrap is independent from its peers, as it does not depend on previous chosen samples when sampling. Then, m models are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification). https://statisticsbyjim.com/hypothesis-testing/bootstrapping/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a3f91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random forest\n",
    "\n",
    "Random forest is a technique that tries to decorrelate the predictors\n",
    "\n",
    "* randomly chosen **subset of input variables (features)**\n",
    "    * leading to different features used at each node\n",
    "     \n",
    "* randomly chosen **subset of data cases (observations)**\n",
    "    * leading to out of bag instances\n",
    "         \n",
    "Random forest gain in performance but lose in interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76446ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grow a forest rather than a tree\n",
    "\n",
    "* Ensemble of decision trees\n",
    "* Trained using bagging for selecting the train data\n",
    "* Uses a random subset of features at each node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211035b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "* For each model, there are instances that are not trained on (because of the replacement). They are called out-of-bag instances (oob).\n",
    "* oobs are ideal for evaluation even without a separate validation set (out of bag error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05997d5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random patches and Subspaces\n",
    "\n",
    "Why not sample the features too?\n",
    "* Random patches: sampling **both** features and instances\n",
    "* Random subspaces: Sample **features only**\n",
    "* Can be used for getting feature importance (How ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb165fd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extra-Trees - Extremely Randomized Trees\n",
    "\n",
    "Changing how we build the trees to introduce more variation\n",
    "* all the data in train is used to build each split\n",
    "* to get the root node or any node: searching in a subset of random features, the split is chosen randomly\n",
    "* random selection → Saves a lot of computational power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24086a89",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Random forest uses bootstrap replicas, that is to say, it subsamples the input data with replacement, whereas Extra Trees use the whole original sample. In the Extra Trees sklearn implementation there is an optional parameter that allows users to bootstrap replicas, but by default, it uses the entire input sample. This may increase variance because bootstrapping makes it more diversified. Another difference is the selection of cut points in order to split nodes. Random Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. Therefore, Extra Trees adds randomization but still has optimization. These differences motivate the reduction of both bias and variance. On one hand, using the whole original sample instead of a bootstrap replica will reduce bias. On the other hand, choosing randomly the split point of each node will reduce variance. In terms of computational cost, and therefore execution time, the Extra Trees algorithm is faster. This algorithm saves time because the whole procedure is the same, but it randomly chooses the split point and does not calculate the optimal one. (https://quantdare.com/what-is-the-difference-between-extra-trees-and-random-forest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0a489",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy vs variance \n",
    "\n",
    "**Accuracy**\n",
    "* Random Forest and Extra Trees outperform Decision Trees\n",
    "\n",
    "**Variance**\n",
    "* Decision trees → high\n",
    "* Random Forest → medium\n",
    "* Extra Trees → low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2eea9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* Machine learning a probabilistic perspective - Kevin p. Murphy\n",
    "* https://arxiv.org/pdf/1106.0257.pdf\n",
    "* https://arxiv.org/pdf/1307.6522.pdf\n",
    "* [Voting classifier blogpost](https://medium.com/@sanchitamangale12/voting-classifier-1be10db6d7a5)\n",
    "* [Difference between extra trees and random forest](https://quantdare.com/what-is-the-difference-between-extra-trees-and-random-fo)\n",
    "* [Model design and selection with sklearn](https://towardsdatascience.com/model-design-and-selection-with-scikit-learn-18a29041d02a)\n",
    "* [Interpretable ML book](https://christophm.github.io/interpretable-ml-book)\n",
    "* [Partial dependency plots](https://www.kaggle.com/dansbecker/partial-dependence-plots)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "rise": {
   "auto_select": "none",
   "transition": "fade"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
