{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bbfaaa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad83c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"slide-title\">\n",
    "\n",
    "# Natural Language Processing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc53f6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: What is NL? Ultimate goal? - Natural human-to-computer communication Sub-field of Artificial Intelligence, but very interdisciplinary - computer science, human-computer interaction (HCI), linguistics, cognitive psychology, speech signal processing (EE), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523faef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/nlp/img_p0_1.png\" width=1000>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623476f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Why care about text?\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "        \n",
    "* Chat bots\n",
    "* Spell checking\n",
    "* Speech recognition\n",
    "* Sentiment analysis\n",
    "* Book recommder\n",
    "* Translators\n",
    "* ...\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/img_p1_1.png\">\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d707f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with text data\n",
    "\n",
    "* Algorithms work well with numbers\n",
    "* working with text = meaningfully transforming your data into numbers\n",
    "* meaningful = depends on your application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747796cc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: How is language text different from numerical/categorical data?  \n",
    "* It's a string, so can't do mathematical operations on it. ('tree' > 'blade of gras' doesn't work.)  \n",
    "* It can be ambigious. (A good life depends on a liver. – Liver may be an organ or simply a living person.)  \n",
    "* It can be in a wide range of formats / files. (oral, UTF-8, handwritten, farsi, ...)  \n",
    "* Has a wide granularity level. (file, paragraph, sentence, word, character)  \n",
    "* It is unstructured. (no mandatory constraint in word order)  \n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a68bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Converting text into numbers\n",
    "\n",
    "* this is also called **text preprocessing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193681e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Text preprocessing also includes cleaning of the raw text before converting into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f93a7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text processing → text to numbers\n",
    "\n",
    "Local representations\n",
    "* Encoding with a unique number\n",
    "* Statistical Encodings\n",
    "\n",
    "Distributed Representations\n",
    "* Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a074ab5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "The term local refers to the uniqueness of a single word in vector space. The words and vectors are a 1-1 relationship (e.g. bag of words).  \n",
    "In distributed representations a word can be represented by several vectors and a vector can mean different (but semantically similar) words (or phrases, in genral: entities) (= many to many)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b51483",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text processing → text to numbers\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "        \n",
    "**Encoding with a unique number**\n",
    "\n",
    "        \n",
    "Easy to create, but the numbers have no relational representation\n",
    "- the relationship between words is not captured\n",
    "- models cannot interpret well these representation\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/img_p6_1.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ade14",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  Drawback of using unique numbers: words with similar meanings can have completely different numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989bf42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Text processing → text to numbers\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "        \n",
    "**Statistical Encodings**\n",
    "\n",
    "Creating vectors of the size of the vocabulary\n",
    "- leads to large sparse features space\n",
    "- not very efficient\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/img_p7_1.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151e415",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text processing → text to numbers\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "        \n",
    "**Word Embeddings**\n",
    "\n",
    "embedding = new latent space    \n",
    "* properties and relationships between items are preserved\n",
    "* less number of dimensions\n",
    "* less sparseness\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/img_p8_1.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f37f3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: \"ice\" and \"cream\" are closer to each other than to \"Mia\".  \n",
    "No further remarks on \"encoding with unique number\" here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622e5b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41165b60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "* Tokenization\n",
    "* CountVectorizer\n",
    "* TF-IDF\n",
    "* N-grams\n",
    "* Normalization\n",
    "* Stemming\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb7a5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/nlp/img_p11_1.png\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d100a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: A token can be a word, a subword or even a character.  \n",
    "The term \"corpus\" (collection of documents) is the equivalent to data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96a6d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbc54e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  nltk is natural language toolkit, developed since 2001 for academic purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7525d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Let us learn some NLP. NLP is amazing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18fefe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88affdd3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0a019",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CountVectorizer\n",
    "\n",
    "Converting a collection of text documents to a matrix of token counts\n",
    "\n",
    "[sklearn's CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce5b15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CountVectorizer\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/nlp/img_p13_4.png\">\n",
    "    <br>\n",
    "    <img src=\"../images/nlp/img_p13_5.png\">\n",
    "</center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "\n",
    "Gives a lot of weight to frequent (and maybe not so informative) words... → TF-IDF fixes this\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45162020",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first Document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b45748",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a88927",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features = cv.get_feature_names_out()\n",
    "print(f\"Features - {features}\")\n",
    " \n",
    "output = pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())\n",
    "print(\"\\n\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693d436",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  This (sparse) matrix is called \"document-term matrix\".  \n",
    "Possibilities to shorten it:  \n",
    "- remove so-called \"stop words\" (see below) which appear very often (e.g. in more than 75% of the documents) and have probably no relevant meaning.  \n",
    "- lowercase all words (with some information loss)  \n",
    "\n",
    "Objective of this matrix:  \n",
    "Can e.g. be used to classify to which document a sentence belongs to (e.g.via logistic regression: every word column is a feature. while the document name could be the label.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264c549",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y = ['document 1', 'document 2', 'document 3', 'document 4']\n",
    "model = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe98368",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "query = ['What is about second document?']\n",
    "\n",
    "query_transformed = cv.transform(query)\n",
    "\n",
    "model.predict(query_transformed)[0]\n",
    "#model.predict_proba(query_transformed)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af186085",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "**TF-IDF**: Term Frequency * Inverse Document Frequency\n",
    "\n",
    "→ measure how important a word is to a document in a corpus\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "\n",
    "A frequent word in a document that is also frequent in the corpus is less important to a document than a frequent word in a document that is not frequent in the corpus.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f026d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  Why is TF-IDF needed if basic document-term matrix works already? Improves prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedfbfec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "**TF**: \n",
    "\n",
    "$$\\text{tf}(t, d)=\\frac{f_{t,d}}{\\sum_{t'\\in{d}} f_{t', d}} $$\n",
    "\n",
    "**IDF**:\n",
    "\n",
    "$$\\text{idf}(t, D)= \\log\\frac{N}{|\\{d\\in{D}:t\\in{d}\\}|}$$\n",
    "\n",
    "**TF-IDF**:\n",
    "\n",
    "$$\\text{tfidf}(t, d, D)=\\text{tf}(t, d) \\cdot \\text{idf}(t, D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318d372",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "TF  \n",
    "Objective: The more frequent a term appears in a document, the more charateristic it is for this document.  \n",
    "TF = occurences of term in document / number of unique terms in document.  \n",
    "There is a TF for every term in every document.  \n",
    "\n",
    "IDF  \n",
    "Objective: If a term appears only in few documents, it has high explanatory power.  \n",
    "IDF = log (number of documents / number of documents containing the term in question)  \n",
    "There is one IDF for every unique term of the corpus.  \n",
    "\n",
    "TF-IDF  \n",
    "Objective: statistical heuristic that takes both into account.\n",
    "TF-IDF = TF * IDF  \n",
    "One for each term in corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0804f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/nlp/img_p13_4.png\">\n",
    "    <br>\n",
    "    <img src=\"../images/nlp/img_p16_3.png\">\n",
    "</center>\n",
    "\n",
    "[sklearn's TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)\n",
    "\n",
    "In detail article how [Tf-IDF](https://medium.com/analytics-vidhya/demonstrating-calculation-of-tf-idf-from-sklearn-4f9526e7e78b) works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506de35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first Document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X = tfidf.fit_transform(corpus)\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4621b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b5e02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### N-grams\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "        \n",
    "To model sequences of words... for example ice and cream make more sense as a 2-gram when they appear together\n",
    "\n",
    "can be at word level or at character level\n",
    "\n",
    "\n",
    "[n-grams](https://books.google.com/ngrams)\n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/nlp/img_p17_2.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef94f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df299940",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "for i in range(1, n):\n",
    "    print(f\"{i} gram\\n\")\n",
    "    ngram = ngrams(text.split(), i)\n",
    "    for gram in ngram:\n",
    "        print(gram)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2acd68",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: using n-grams with large n somehow teaches to understand contexts but dramatically enlarges document-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac323b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalization\n",
    "\n",
    "[‘List’, ‘listed’, ‘lists’, ‘listing’, ‘listings’, ‘.’]\n",
    "\n",
    "→  [‘list’, ‘listed’, ‘lists’, ‘listing’, ‘listings’, ‘.’]\n",
    "\n",
    "\n",
    "Do we want to distinguish between “List” and “list”?\n",
    "\n",
    "Sometimes we do: “White House” vs. “white house”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959f918",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notes: Normalization is the process of converting text data into a standardized form to reduce complexity and improve the efficiency of machine learning models. This can include lowercasing, stemming/lemmatization, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966c90f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming\n",
    "\n",
    "[‘list’, ‘listed’, ‘lists’, ‘listing’, ‘listings’, ‘.’]\n",
    "\n",
    "→ [‘list’, ‘list’, ‘list’, ‘list’, ‘list’, ‘.’]\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "\n",
    "Stemming reduces words to a shorter form, a form that might have no meaning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e5b1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "[‘list’, ‘listed’, ‘lists’, ‘listing’, ‘listings’, ‘.’]\n",
    "\n",
    "→ [‘list’, ‘listed’, ‘list’, ‘listing’, ‘listing’, ‘.’]\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "\n",
    "Lemmatization uses the language dictionary to get the base word of a word.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d612e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "text = \"We are learning how a stemmer works\"\n",
    "text1 = \"People are running so fast.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475732dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(text)\n",
    "stem = [stemmer.stem(word) for word in tokenized_text]\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06acd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705746bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(text)\n",
    "lemm = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d0ef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming or Lemmatization?\n",
    "\n",
    "It depends...\n",
    "* Stemming is faster\n",
    "* Lemmatization preserves more information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f055c5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "stemming works by using a heuristic rules rather than using a lookup table. Pro: Doesn't need a lookup table. Con: stemmed words often don't have any meaning.  \n",
    "Lemmatization usually needs a language specific lookup table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84771458",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## BUT What about meanings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2677e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Advanced Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2949ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: BREAK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52f025",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "* some words do not provide meaningful information ... they are not “content words”\n",
    "* the list of non-content words is language specific and corpus specific\n",
    "\n",
    "What would you say are stop words in this text?\n",
    "\n",
    "\"Apple is looking at buying U.K. startup for $1 billion\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d1d58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "* some words do not provide meaningful information ... they are not “content words”\n",
    "* the list of non-content words is language specific and corpus specific\n",
    "\n",
    "What would you say are stop words in this text?\n",
    "\n",
    "\"Apple **is** looking **at** buying U.K. startup **for** $1 billion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dc6de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e7591",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS Tagging\n",
    "\n",
    "* **P**art **O**f **S**peech tagging - assigning grammatical annotations\n",
    "    * ADJ - adjective\n",
    "    * NOUN\n",
    "    * VERB\n",
    "    * ...\n",
    "    \n",
    "Which are verbs and nouns here?\n",
    "\n",
    "\"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "[universaldependencies](https://universaldependencies.org/docs/u/pos/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65eafff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS Tagging\n",
    "\n",
    "* **P**art **O**f **S**peech tagging - assigning grammatical annotations\n",
    "    * ADJ - adjective\n",
    "    * NOUN\n",
    "    * VERB\n",
    "    * ...\n",
    "    \n",
    "Which are **verbs** and *nouns* here?\n",
    "\n",
    "\"*Apple* is  **looking** at **buying** U.K. *startup* for $1 billion\"\n",
    "\n",
    "[universaldependencies POS](https://universaldependencies.org/docs/u/pos/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7c259",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe51b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(text)\n",
    "tag = pos_tag(tokenized_text)\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad37845",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* VBP - Verb, non-3rd person singular present\n",
    "* VBG - Verb, ending in '-ing' or present participle\n",
    "* VBZ - Verb, 3rd person singular present\n",
    "* WRB - Wh-adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d3173",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS Tagging using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92691d8d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: SpaCy is a more modern language library made for production. Some overlap with nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc0ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e5f4e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "  \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92584d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# new_text = \"The car is blue\"\n",
    "doc = nlp(text)\n",
    "  \n",
    "# Token and Tag\n",
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb4136",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Named Entities \n",
    "\n",
    "\n",
    "* Named Entities are real-world objects that are assigned a name: person, country, book, product..\n",
    "* The recognition of entities is based on training data so it's not perfect.\n",
    "\n",
    "What entities do you think are in this text?\n",
    "\n",
    "\"Apple is looking at buying U.K. startup for $1 billion\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acf024",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Named Entities \n",
    "\n",
    "\n",
    "* Named Entities are real-world objects that are assigned a name: person, country, book, product..\n",
    "* The recognition of entities is based on training data so it's not perfect.\n",
    "\n",
    "What entities do you think are in this text?\n",
    "\n",
    "\"**Apple** is looking at buying **U.K.** startup for $1 billion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fd3de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\" - \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6026b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# GPE above is Geographical Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9a4de",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4891d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562af696",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761c2da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So.. what do we do with all that?\n",
    "\n",
    "* document similarity\n",
    "* text classification\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7e788",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text similarity or Document Similarity\n",
    "\n",
    "Each document is a vector of features. \n",
    "\n",
    "Similarity between documents is the similarity between vectors\n",
    "\n",
    "Usage:\n",
    "* search engines: query to document\n",
    "* clustering of documents: document to document\n",
    "* Question & Answering platforms: query to query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089d334",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text classification\n",
    "You can use your favourite classifier with text\n",
    "* Logistic Regression provides nice baseline\n",
    "* AUC score as performance metric\n",
    "\n",
    "Some applications:\n",
    "* spam detection\n",
    "* sentiment analysis\n",
    "* hate speech analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029fc06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32798d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings\n",
    "* Represent feature space in smaller dimension\n",
    "* Similar words are near in embedding space\n",
    "* Trained by using neural networks  \n",
    "    &emsp;&rarr; Use those trained weights as first layer in your NLP neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2900c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: A common dimensionality of word embeddings is e.g. 300 while English has around 170,000 words.  \n",
    "\n",
    "Using word embeddings transforms words into sensible numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8344c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word similarity\n",
    "Is “St Pauli” more similar to:\n",
    "\n",
    "* De Wallen → Similar type\n",
    "\n",
    "or\n",
    "\n",
    "* HSV → Similar topic?\n",
    "\n",
    "Result depends on the context ... or on the feature space / embedding you chose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfe2a1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "St. Pauli is a district in Hamburg and a name of a Hamburg football club.  \n",
    "De Wallen is a district in Amsterdam.  \n",
    "HSV is a Hamburg football club.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96751de4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Embeddings\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "        \n",
    "Relevant items for your task should be similar in the embedding space / i.e close to each other.\n",
    "        \n",
    ".\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/img_p8_1.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a153e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we get Word Embeddings\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "        \n",
    "Having lots of data and:\n",
    "* Read the text\n",
    "* Process text\n",
    "* Create x, y data points - for example each 2 words appearing in a text\n",
    "* Create one hot encodings\n",
    "* Train a neural network\n",
    "* Extract the weights from the input layer\n",
    "\n",
    "[Example 1](https://github.com/Eligijus112/word-embedding-creation), \n",
    "[Example 2](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)\n",
    "    </div>\n",
    "    <div class=\"text\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179b5c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we get Word Embeddings\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "CBOW - Continuous Bag of Words\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/CBOW.png\">    \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed5ed0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  If we finished training on all context windows (here: 4) of all documents of the corpus, the weigths of the hidden layer (n weights per unique word) are the vectors of the unique words.  \n",
    "(The number of the nodes of the input as well as of the output layer is the number of unique words/tokens.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34976c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we get Word Embeddings\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\">\n",
    "Skip-Gram\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/SkipGram.png\">    \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bcefc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "skip-gram works better than CBOW but needs more input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c134b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we get Word Embeddings\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/img_p39_3.png\">    \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/nlp/img_p39_4.png\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "[Creating word embeddings](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd37c3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "In the list of sentences above, we see that words appearing together in the sentences appear nearby \n",
    "each other in the embeddings graph.\n",
    "Masculine words form one cluster and feminine words form other cluster.\n",
    "It tells us in a small example that words appearing together in a context will be closer to each other in \n",
    "the n-dimensional embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666c7ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using pre-trained embeddings\n",
    "Most times you do not have enough data to get good word embeddings for your task, instead you can use pre-trained word embeddings.  \n",
    "\n",
    "There are different kinds of word embeddings:  \n",
    "- static word embeddings: Word2vec (google), GloVe (Standford University), fastText (Facebook),  \n",
    "- contextual word embeddings: ELMo, Bert (google), gpt-2/3/4 (openAI), ...  \n",
    "\n",
    "\n",
    "example: [pretrained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd2ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "gensim is not a word embedding but a library to handle word embeddings  \n",
    "word2vec and fasttext use skip-gram / CBOW.  \n",
    "fasttext uses parts of words instead of words.   \n",
    "glove works on co-occurence matrix, not on CBOW or skipgram.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624a14b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab25cf5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install scipy==1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef096ade",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "## List available embeddings\n",
    "info = api.info()\n",
    "\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051cd9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# caveat: If you don't have enough RAM, this cell can crash your kernel\n",
    "\n",
    "wv = api.load(\"word2vec-google-news-300\")\n",
    "glove = api.load(\"glove-twitter-100\")\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac7d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the first 200,000 words from the downloaded file only instead\n",
    "wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4532db3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  download 'GoogleNews-vectors-negative300.bin.gz' first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc7751",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(\"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a7d01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wv.get_vector(\"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1a5da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "glove.most_similar(\"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5abc00",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fasttext.most_similar(\"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a35bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wv.distance(\"coffee\", \"tea\")\n",
    "# wv.distance(\"coffee\",\"coffees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd58df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wv.distance(\"coffee\", \"onion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f5a69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0dc351",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: \"most_similar\" because it is very unlikely that another word has exactly the 300 components of the difference vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d31f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=[\"restaurant\", \"coffee\"], negative=[\"dinner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba55e44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=[\"Berlin\", \"France\"], negative=[\"Germany\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.doesnt_match([\"sklearn\",\"numpy\",\"python\",\"pandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658efcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out which other methods there are and test their function\n",
    "dir(wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d50ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize Semantics with Graphs \n",
    "\n",
    "[TensorFlow projector](https://projector.tensorflow.org)\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/nlp/img_p41_2.png\" width=1100>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f01a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Hugging Face & Transformers\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/nlp/img_p42_1.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539eefd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Hugging Face\n",
    "\n",
    "~ 7k pre trained NLP models on [huggingface.co](https://huggingface.co)\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/nlp/img_p43_1.png\" width=800>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56284f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Zero Shot Learning \n",
    "\n",
    "When you have little data.\n",
    "\n",
    "**Zero-shot learning (ZSL)** is a problem setup in machine **learning**, where at test time, a learner\n",
    "observes samples from classes that were not observed during **training**, and needs to predict the\n",
    "class they belong to.\n",
    "\n",
    "(see notebook 2 in workbooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2dd9c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "Zero Shot Classification is the task of predicting a class that wasn't seen by the model during training. Zero-shot text classification is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.  \n",
    "It's also called heterogeneous transfer learning.  \n",
    "This works because those models use auxiliary information, e.g. from a text corpus (keyword: multimodel inputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05023036",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Getting started with NLP (Pyladies)](https://github.com/pyladieshamburg/getting-started-with-nlp)\n",
    "- [NGram Loader](https://pypi.org/project/google-ngram-downloader/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [Text similarities](https://medium.com/@adriensieg/text-similarities-da019229c894)\n",
    "- [Neural models for information retrieval](https://www.microsoft.com/en-us/research/video/neural-models-information-retrieval-video/)\n",
    "- [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "- [What is a transformer? (3blue1brown)](https://www.youtube.com/watch?v=wjZofJX0v4M)\n",
    "- How does zero shot learning work [[video](https://blog.roboflow.com/zero-shot-learning-computer-vision/), [text](https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html)]?\n",
    "- Sentiment Analysis with VADER [[stand alone](https://vadersentiment.readthedocs.io/en/latest/), [using nltk](https://www.nltk.org/howto/sentiment.html)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59296b1d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  \n",
    "Maybe show simple example of sentiment analysis (instead of some other slides)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "auto_select": "none",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
