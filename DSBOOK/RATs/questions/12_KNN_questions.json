[
    {
        "question": "Check all statements on KNN that are correct.",
        "type": "many_choice",
        "answers": [
            {
                "answer": "KNN can be used for classification.",
                "correct": true,
                "feedback": "That's correct! KNN is a supervised learning algorithm and can be used for classification. "
            },
            {
                "answer": "KNN can be used for regression.",
                "correct": true,
                "feedback": "That's correct! KNN is a supervised learning algorithm and can be used for regression. "
            },
            {
                "answer": "KNN is an unsupervised learning algorithm.",
                "correct": false,
                "feedback": "Sorry, that was wrong. KNN is a supervised learning algorithm. Please try again"
            },
            {
                "answer": "KNN has longer training time than prediction time.",
                "correct": false,
                "feedback": "Sorry, that was wrong. At training time, KNN doesn't need to do distance calculations, while the prediction time is high, as distances between test data and training data points have to be calculated (to decide which are its closest neighbors). Please try again"
            },
            {
                "answer": "KNN makes assumptions about the functional form of the problem being solved.",
                "correct": false,
                "feedback": "Sorry, that was wrong. KNN is a non-parametric algorithm and does not make any assumption on data distribution! Please try again"
            }
        ]
    },
    {
        "question": "Is it important to scale the data when applying KNN? What do you answer?",
        "type": "multiple_choice",
        "answers": [
            {
                "answer": "No, KNN will find the nearest neighbors despite different scales.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            },
            {
                "answer": "Yes, it is computationally more efficient. The results would be the same, but Gradient Descent will find the minimum faster.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            },
            {
                "answer": "It depends. You don't need to scale when using Euclidean distance. For Manhattan distance you should use scaled data.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            },
            {
                "answer": "Yes, KNN is based on similarity measurement,  which is sensitive to scaling.",
                "correct": true,
                "feedback": "Correct. Algorithms based on similarity measurements are sensitive to scaling. The similarity is measured by calculating distances between points. If the features don't have the same scale, the feature with the larger scale will have a bigger influence of the outcome than smaller scaled features. To avoid this, you should scale your data before using KNN."
            }
        ]
    },
    {
        "question": "When you find noise in data which of the following option would you consider in KNN?",
        "type": "multiple_choice",
        "answers": [
            {
                "answer": "I will increase the value of k.",
                "correct": true,
                "feedback": "That's right. To be more sure of which classifications you make, you can try increasing the value of k."
            },
            {
                "answer": "I will decrease the value of k.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            },
            {
                "answer": "Noise can not be dependent on value of k.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            },
            {
                "answer": "None of these.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            }
        ]
    },
    {
        "question": "Which of the following will be true about k in KNN in terms of Bias?",
        "type": "multiple_choice",
        "answers": [
            {
                "answer": "When you increase the k the bias will be increased.",
                "correct": true,
                "feedback": "Choosing large k means simplify the model and simple model tend to have high bias."
            },
            {
                "answer": "When you decrease the k the bias will be increased.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            },
            {
                "answer": "Can't say.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            },
            {
                "answer": "None of these.",
                "correct": false,
                "feedback": "Sorry, that was wrong. Please try again"
            }
        ]
    },
    {
        "question": "Your colleague believes he is overfitting his KNN model. How could you check that the model is overfitting? Which parameter can you change (and how) to reduce overfitting?",
        "type": "multiple_choice",
        "answer_cols": 1,
        "answers": [
            {
            "answer": "Click here to see the answer.",
            "correct": true,
            "feedback": "A model is overfitting, if it cannot generalize well on unseen data. Therefore, you should test model performance on an unseen test data set. If the error on the test set is much higher than the error on the train data, your model might be overfitting. If you want to reduce overfitting using KNN, you need to increase the hyperparameter k. With increasing k, the decision boundary gets less flexible and less responsive to variance in the training data."
            }
        ]
    }
]