{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f502c089",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Image Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa8e1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-title\">\n",
    "\n",
    "# Image Modeling\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93239732",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: A specific type of neuron was developed for working with image data - a convolutional neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2d13e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Warm-up üå∂Ô∏è\n",
    "Spend some time looking at this explanation of [Image Kernels](https://setosa.io/ev/image-kernels/) (feel free to play around with all the settings!) and discuss the following questions:\n",
    "\n",
    "+ What does the number at each pixel location in a greyscale image mean?\n",
    "+ What is a kernel?\n",
    "+ How is a kernel applied to an image?\n",
    "+ How does changing the numbers in the kernel affect the output image?\n",
    "+ What features of an image can we highlight using a kernel?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2e9a3",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5a2d4",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Image Data\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "        \n",
    "* There is possibly no other data that has increased as much over the last decade - and filled our everyday life - as image data\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee841c",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* In a single day on Instagram 1.3 billion photos and videos are uploaded (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37419ec1",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* We want to process such data for many reasons\n",
    "    * Image classification and captioning\n",
    "    * Toxic content detection\n",
    "    * Image quality improvements, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee3a1c",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Image data is everywhere and it invites an automatic procedure for analysing this data, hence image modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808df217",
   "metadata": {
    "hideOutput": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd69327b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does image data look like?\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">\n",
    "        \n",
    "* Image data has height, width and depth\n",
    "    * Height and width are given in pixels\n",
    "    * Depth is the number of channels (3 for **R**ed**G**reen**B**lue, 1 for **B**lack**W**hite)\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/image_example.png\">     \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b71ad5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Image data is encoded using 3 channels (we can think of it as a 3 dimensional picture) - Red, Green and Blue. On each layer a pixel defines the strength of the red, green, blue colour. Satellite images can have around 9 layers (infrared etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda0536",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Each pixel corresponds to an integer between 0 and 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad9950",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Example: A **1280 x 720** px color image has **1280 x 720 x 3** = **2,764,800** pixels = **2.76** MB\n",
    "    * That is a lot to digest for an ML model using thousands of images for training and maybe hundreds in a batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880a9e5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The 3 channels and the large amount of information conveyed in each image is different from tabular data we have dealt with before. Image data is unstructured by its nature and that is very much different from structured, tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4897c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unstructured data and locality\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\">  \n",
    "\n",
    "* In contrast to structured table data images show regularly transformations in local features\n",
    "    * Different angles, rotations, flipping, scaling, translations\n",
    "* Humans can see the same objects just naturally\n",
    "* For an ML model this is a challenge\n",
    "    * Transformation invariance is desired\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/fish.png\" width=\"90%\">     \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de59bf",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: ‚ÄúTransformation invariance‚Äù - seeing the same object from different angles is easy for human beings, but needs to be constructed for Machine Learning. Shearing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05d281",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principle of locality\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "        \n",
    "* Nearby pixels often show the same or a similar color\n",
    "* As pixels become further apart this similarity decreases or even breaks\n",
    "* Locality describes this phenomenon:\n",
    "    * Locally pixels are correlated\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/img_p11_2.png\">     \n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad6b75",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Locally pixels are correlated and globally they are not. Shearing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0162e",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Structured and unstructured data\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* With structured data we would simply compare two feature vectors\n",
    "    * Example: Structured truck data\n",
    "        \n",
    "        \n",
    "</div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/img_p13_3.png\">  \n",
    "        \n",
    "\n",
    "        \n",
    "| Height (in m) | Weight (in m) |\n",
    "|---------------|---------------|\n",
    "| 2.420 | 11 | \n",
    "| 2.849 | 28 |        \n",
    "| 2.975 | 50 |        \n",
    "\n",
    "[Source](https://www.thelocal.de/20110217/33185)\n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae47534",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Comparing 3 observations of the trucks is a specific case of measuring distance in vector space (Height being 1 dimension of a vector and Weight being the other). There are different norms (distance metrics) we could use to compute the distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4bad3",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Structured and unstructured data\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* With structured data we would simply compare two feature vectors\n",
    "    * Example: Structured truck data\n",
    "        \n",
    "Comparing the euclidean distance:\n",
    "\n",
    "$(2.849-2.420)^{2}+(28-11)^{2}=289.19$ \\\n",
    "$(2.975-2.420)^{2}+(50-11)^{2}=1,521.31$\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "</div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/img_p13_3.png\">  \n",
    "        \n",
    "\n",
    "        \n",
    "| Height (in m) | Weight (in m) |\n",
    "|---------------|---------------|\n",
    "| 2.420 | 11 | \n",
    "| 2.849 | 28 |        \n",
    "| 2.975 | 50 |        \n",
    "\n",
    "[Source](https://www.thelocal.de/20110217/33185)\n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da309c2",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: In this case we have decided to use Euclidean distance and we could see that the difference between Truck1 and Truck2 is smaller than between Truck2 and Truck3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34aa5ac",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Structured and unstructured data\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* With structured data we would simply compare two feature vectors\n",
    "    * Example: Structured truck data\n",
    "\n",
    "* This does not work with image data comparing the vectors of all pixels (rolling them out)\n",
    "    * The reason is locality (more generally transformation)\n",
    "\n",
    "        \n",
    "</div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/img_p13_3.png\"> \n",
    "        <img src=\"../images/image_modelling/fishes.png\" width=\"80%\">        \n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc092e",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: If we tried to apply the same procedure to images by converting data into a vector representation we would conclude that the 2 images we see are very different from each other. This kind of distance measurement doesn‚Äôt work on images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310aaf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is ideal feature information?\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "        \n",
    "* The primary information to identify objects in an image are the pixel relations\n",
    "    * Pixels distributed in a certain way identify a certain object\n",
    "    * Colour compositions play in certain areas a crucial role\n",
    "* But how can we extract features that contain this information from an image?\n",
    "    * This is a demanding task\n",
    "* How do we humans identify a fish as a fish?\n",
    "        \n",
    "[Source](https://www.sciencedirect.com/science/article/pii/S0031320316303582?via%3Dihub)\n",
    "        \n",
    "</div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/img_p16_3.png\" width=300> \n",
    "        <img src=\"../images/image_modelling/img_p16_4.png\" width=300>\n",
    "        <img src=\"../images/image_modelling/pixles.gif\">\n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd5059",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The randomly distributed pixels we see do not contain any meaningful object. The distribution of the pixels of real objects in nature are very different from the randomly distributed pixels. What we recognise is a distribution of pixels that we call the shape. 2nd image of the right - ML model gives more weight to these pixels which form the edge and which in turn help in recognising objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f6867",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Image feature engineering\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "        \n",
    "* Earlier approaches to image processing relied heavily on feature engineering by hand\n",
    "    * Using edges to identify shape: edges are sharp changes in color\n",
    "* Some popular image features developed at these times\n",
    "    * HOG (Histogram of Oriented Gradients) - Looking at changes in pixels\n",
    "    * SIFT (Scale Invariant Feature Transform)\n",
    "    * SURF (Speeded-Up Robust Feature)\n",
    "        \n",
    "</div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/img_p17_2.png\"> \n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504c91e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Detecting the shape means focusing on the edges and earlier methods relied on feature engineering by hand to detect the edges. We won‚Äôt be focusing on feature engineering by hand because this method proved to be not very successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a07529",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The ideal feature information\n",
    "\n",
    "* As an ideal, we want the model itself to find the best features to fulfill its task\n",
    "* How can a model map the high-dimensional image space to a lower dimensional subspace?\n",
    "    * In which each image is represented by a long feature vector that behaves like structural data\n",
    "* In the following we try to find a model that can achieve this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789aa89",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Moving from a high-dimensional to a lower dimensional subspace allows us to separate between different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5387c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear models for image modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9dd93b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to prepare the image data for a linear model?\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* Given an image with height and width:\n",
    "    * We could flatten the 2-dimensional tensor into a 1-dimensional one\n",
    "    * Then just concatenating the channels therein\n",
    "* Given the label to be one of [T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot]:\n",
    "    * The label is represented by an integer between 0 and 9\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/flattend_2.png\"> \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5ec9f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The MNIST fashion dataset was used for a long time in an image classification challenges. It contains pictures of different clothing items and labelled by hand to specify the class. At first the images were black and white and later on they became coloured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40062217",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* The logistic classification works on image data\n",
    "    * On the MNIST Fashion dataset it reaches a 86% accuracy\n",
    "    * That is almost a 15% error that could become costly in business\n",
    "\n",
    "* Can we improve modeling these image data?\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/flattend.png\"> \n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827bc3f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Input layer + Output-Layer with 10 neurons, sigmoid activation and cross-entropy loss. Output layer acts as a LogReg per neuron as they have sigmoid activation. For MNIST this error is not too large, but given how constricted and simple this dataset is this level of accuracy is not enough. In case of medical images for example it would be too low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3dc11f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Drawbacks of linear models\n",
    "\n",
    "* Remember the TensorFlow playground\n",
    "    * Linear models perform bad on non-linear data\n",
    "    * Complex structures in data are hard to capture for linear boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a66931",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Image data is inherently non-linear - edges and borders mark differences between objects and the colour changes from object to another are sudden and not linearly predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784bfbaa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We need models capable of complex relationships between pixels\n",
    "    * They must be able to approximate non-linear functions of high complexity\n",
    "    * Such models must be able to do the feature engineering themselves\n",
    "\n",
    "\n",
    "[TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=10&networkShape=4,1&seed=0.52914&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7317b86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Neural Networks for image modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9986b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Deep Neural Networks perform better on image data\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* Remember\n",
    "    * Non-linear activation functions enable networks to model nonlinear functions\n",
    "    * Hierarchy of neurons through layers make a complex feature extraction possible \n",
    "\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/dnn.png\"> \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01485613",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Hierarchy of neurons and the resulting feature extraction enables us to deal with complex data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dde62c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Neural Networks perform better on image data\n",
    " \n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* Remember\n",
    "    * Non-linear activation functions enable networks to model nonlinear functions\n",
    "    * Hierarchy of neurons through layers make a complex feature extraction possible \n",
    "\n",
    "* Deep Neural Network on MNIST Fashion gets to a 91% accuracy\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/dnn.png\"> \n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6406d4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: 91% accuracy is an improvement, but considering how simple of a dataset MNIST is it‚Äôs still too low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d08008",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Drawbacks of Deep Neural Networks in image modeling\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "\n",
    "* Theoretically we can build a very deep network and approximate any function to classify images (universal approximation theorem), but\n",
    "    * Very deep networks have **high risk of overfitting**\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/dnn_small.png\"> \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264393a3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Bias reduced, but higher variance; Size of the network can become too large to train - even for a single GPU (memory); Order of pixels is highly relevant for us - randomly shuffling looks like noise to us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53c83f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Drawbacks of Deep Neural Networks in image modeling\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "\n",
    "* Theoretically we can build a very deep network and approximate any function to classify images (universal approximation theorem), but\n",
    "    * Very deep networks have **high risk of overfitting**\n",
    "        \n",
    "    * Real-world images have way more pixels \n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/dnn_small.png\"> \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc498fe2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Bias reduced, but higher variance Size of the network can become too large to train - even for a single GPU (memory) Order of pixels is highly relevant for us - randomly shuffling looks like noise to us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff54858",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Drawbacks of Deep Neural Networks in image modeling\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "\n",
    "* Theoretically we can build a very deep network and approximate any function to classify images (universal approximation theorem), but\n",
    "    * Very deep networks have **high risk of overfitting**\n",
    "        \n",
    "    * Real-world images have way more pixels \n",
    "        \n",
    "    * DNNs are not transformation invariant but are not dependent on the order of pixels either \n",
    "        * i.e. the relationship between pixels does not count\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/dnn_small.png\"> \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7ac6d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Deep Neural Networks are highly random in their training and disregards the order of the pixels. In the case of images order of pixels matters. Bias reduced, but higher variance Size of the network can become too large to train - even for a single GPU (memory) Order of pixels is highly relevant for us - randomly shuffling looks like noise to us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ddfa8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ImageNet Competition\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "\n",
    "* ImageNet is the largest image recognition competition\n",
    "    * ~1.2M images and 1,000 categories\n",
    "* Until 2011 feature engineering defined a dominant part for winning teams\n",
    "    * Xerox XRCE team won by using Fisher Vectors (multi-dimensional SWIFT) and SVMs\n",
    "* 2012 AlexNet improved significantly and presented an implicit feature extraction\n",
    "    * The authors used a {convolutional neural network (CNN)}\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/cnn.png\"> \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73605e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Convolutional layers were a major breakthrough in image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c940908",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777a687",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* A convolutional neural network consists of two special layers:\n",
    "    * Convolutional layers\n",
    "    * Pooling layers\n",
    "* In the following we will consider these layers in detail\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/cnn_detail.png\"> \n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df619b6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: AT&T 1995 split into NCR, Bell Labs, AT&T Technologies. Patent (2007) to NCT and Scientists to Bell Labs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca95d5",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Convolution as a grouping of information\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* We found out that the local relationships between pixels in an image contain important information  \n",
    "    * How can this relationship be captured?\n",
    "* Convolutional layers use a special mathematical method to capture these relationships: a **{convolution}**\n",
    "    * Intuitively, they take snapshots from all regions of an image and apply filters to them\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/convolution.png\"> \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab19d7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The aim is to extract information from some local regions. Convolutional layers take snapshots of images. Convolution takes a function and slides it over another function integrating (infinitely summing) both as it moves along. In this way the form of the function is captured - in the middle image the function peaks at a point where 2 functions cover each other. In a way it serves as a filter that is helping detect a certain shape, something that can be very helpful in an image. Convolution: How is the shape of a function modified by another? Shaded region: Product of f and g as a function of t Three carts with doses and 5 patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2f685",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### How does a convolution layer work?\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "        <img src=\"../images/image_modelling/convolution_process.png\">    \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa38075",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: You slide the Kernel (filter) over the Image and perform cross-correlation as demonstrated in the slide. The result of 1 (in orange) is like a snapshot of the region. Convolution is a historical name that is used, but in reality we use cross-correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d792d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does a convolution layer work?\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "        <img src=\"../images/image_modelling/convolution_process.png\">    \n",
    "    </div>\n",
    "    <div class=\"images_30\">\n",
    "<br>\n",
    "        \n",
    "\n",
    "$$ y_{i,j}=\\sum_{k}\\,\\sum_{l}\\,w_{k l}\\,x_{i+k,j+l}$$\n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4512a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: In reality we use cross-correlation rather than convolution. i,j - indices of the image rows and columns k,l - indices of the kernel rows and columns The kernel is moved by 1 pixel each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fedda1",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Edge detection\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* A filter can detect different features in an image\n",
    "    * Edges\n",
    "    * Sharpening (edge detection + original image)\n",
    "    * Blurring\n",
    "* Here we only look at edge detection to get an intuition how filtering works\n",
    "\n",
    "<img src=\"../images/image_modelling/calculation.png\"  width=600> \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/fish_calculation.png\" width=500 > \n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6988a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Using different features allows us to emphasise various types of elements in the image - in the example above the vertical and horizontal features are emphasised. The output of a filter gives a signal where a certain type of feature in the image might be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef06663",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Major idea of a convolutional layer\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* Filters can be constructed by hand\n",
    "    * Why not learning filter values {parameters} in training?\n",
    "* A convolutional layer contains many filters\n",
    "    * Each filter forms during training\n",
    "* Each filter is of the same size and can process groups of nearby pixels\n",
    "    * It looks at relationships between pixels {locality}\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/convolutional_layer.png\" width=550> \n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e827dfd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: A convolutional layer learns the filters in order to fulfill the task the best possible way. The number filters defines how many layers there are in the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1d7ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduction of dimensions\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* Convolutional layers reduce the dimension of the image\n",
    "    * A 28 x 28 picture filtered by a 3 x 3 kernel shrinks to a dimension of 26 x 26\n",
    "* This limits the number of convolutional layers applied to an image\n",
    "    * At each layer the image shrinks\n",
    "* Formula for the resulting image dimension:\n",
    "        \n",
    "$$m=n-k+1$$\n",
    "$$28-3+1=26$$\n",
    "    \n",
    "</div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/convolutional_gif.gif\">\n",
    "        \n",
    "[Source](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/#:~:text=Left:%20the%20filter%20slides%20over%20the%20input.%20Right:%20the%20result)\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3941b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Shrinking the image at each convolutional step means that eventually we will not be able to apply filters anymore. Hence it won‚Äôt be possible to build deep convolutional neural networks on an original image and we would need to use ‚Äúpadding‚Äù. n - number of dimensions in the original image k - number of dimensions in the kernel m - number of dimensions in the new image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b6dc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we build deep convolutional networks?\n",
    "\n",
    "* To avoid shrinking the image too quickly **{padding}** is applied\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0cc3f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: ‚ÄúSame‚Äù Padding means same output size as image size Pixel always in the center ‚ÄúValid‚Äù padding is actually ‚Äúno padding‚Äù - image gets fully covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932bd69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* The image is ‚Äòframed‚Äô by zeros\n",
    "     * A 28 x 28 image with a padding of 1 has dimension 30 x 30\n",
    "     * WIth a kernel of 3 x 3 this results again in a 28 x 28 image\n",
    "* Formula for the resulting image size\n",
    "\n",
    "$$m=n+2p-k+1$$\n",
    "$$28+2*1-3+1=28$$\n",
    "        \n",
    "</div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/kernel.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a2159",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: n - number of dimensions in the original image k - number of dimensions in the kernel m - number of dimensions in the new image p - number of dimensions of padding ‚ÄúSame‚Äù Padding means same output size as image size Pixel always in the center ‚ÄúValid‚Äù padding is actually ‚Äúno padding‚Äù - image gets fully covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f738f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making great strides\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text_70\"> \n",
    "\n",
    "* Stride is the step size of the filter {kernel} by which it moves along the image\n",
    "* Sometimes a larger stride is desired\n",
    "    * It reduces large image sizes to optimize memory usage and reduces computational costs\n",
    "    * It reduces the risk of overfitting\n",
    "* Formula for the resulting image dimension (stride of 2):\n",
    "\n",
    "$$m=\\big\\lfloor{\\frac{n+2p-k}{s}}+1\\big\\rfloor$$\n",
    "$$m=\\big\\lfloor{\\frac{28+2\\cdot0-3}{2}}+1\\big\\rfloor=13$$\n",
    "\n",
    "</div>\n",
    "    <div class=\"images_30\">\n",
    "        <img src=\"../images/image_modelling/stride_2.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517e31e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Stride size sets by how many pixels we move the filter each time. n - number of dimensions in the original image k - number of dimensions in the kernel m - number of dimensions in the new image p - number of dimensions of padding s - stride size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353cc90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What happens during training?\n",
    "\n",
    "* The filters {kernels} of a convolutional layer are variable\n",
    "    * They get learned during training of the network\n",
    "* The network learns thereby optimal filters to fulfill its task\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/image_modelling/feature_map.png\" width=1000>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b66d9f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The kernels contain weights that are being trained and form the filters that are meaningful for the task. The size of the filter is a hyperparameter that depends on the specific domain. The stride is usually not learnt, but is set as a hyperparameter that depends on the specific domain. Padding of 0 (and not some other value) allows us to ensure we don‚Äôt induce a signal that wasn‚Äôt there anywhere. Images are typically converted to the same dimension, but convolutional layer is not restricted by the dimensions as it can slide over the image of any size (with the dimensions bigger than the filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a808b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stationarity principle\n",
    "\n",
    "* There are features in an image that are essential and repeat themselves at different locations\n",
    "* Statistical signals are uniformly distributed:\n",
    "    * These features have therefore also be detected at each location\n",
    "* This justifies {parameter sharing} which makes CNNs very parameter-efficient \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133797b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Stationarity means that filters can be reused - i.e. vertical features can be detected by the same filter in different areas of the image. In other words convolutional Neural Network can learn certain parameters in one area of the image and apply it to another area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fede5d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Working through the channels\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* Color images have three channels\n",
    "    * Convolutions take place across channels\n",
    "    * I.e. a filter {kernel} is actually a cube\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/rgb.jpeg\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729a6a2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: The filter is like a cube that is applied to all channels at once and a single feature layer is output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7a473",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Working through the channels\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* Color images have three channels\n",
    "    * Convolutions take place across channels\n",
    "    * I.e. a filter {kernel} is actually a cube\n",
    "* Per convolutional layer there are usually many filters applied\n",
    "    * A filter per feature\n",
    "    * I.e. the output is actually again multi-channel\n",
    "    * The channel of the output is also called {feature map}\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/rgb_2.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e169b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: With 2 filters 2 Feature Maps will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0688f8b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameter sharing is caring\n",
    "\n",
    "* Remember the large image example (1280px x 720px):\n",
    "    * 27.6 Mio. parameters with 10 neurons in a DNN\n",
    "    * 110 MB in memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3dc491",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Training the network above is possible, but will take a long time and will not result in a well performing network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade937da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consider instead a convolutional layer with 64 filters of size 3x3 applied to a colour image\n",
    "    * 3x3x3x64 + 64= 1792 parameters!\n",
    "    * 7 KiB in memory\n",
    "    \n",
    "* With parameter sharing we can train\n",
    "    * faster\n",
    "    * deeper networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205b5ec",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Because image data is stationary parameter sharing works. Otherwise the number of parameters and the large size of the input would mean that convolutional neural networks would take forever to train and would not work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e60e4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b384177",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Pooling layers are not trained, they just aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734aff9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is pooling?\n",
    "\n",
    "* Pooling is aggregating features from {feature maps}\n",
    "    * We pool over a certain window: we apply a summary of nearby pixels\n",
    "    * The window is then moved across a feature map\n",
    "* Pooling reduces a feature map in size:\n",
    "    * A 26 x 26 feature map with a 2x2 pooling layer reduces the feature map to dimension 25x25\n",
    "* Formula for the resulting dimension:\n",
    "\n",
    "$$m=\\big\\lfloor{\\frac{n+2p-k}{s}}+1\\big\\rfloor$$\n",
    "\n",
    "$$m=\\big\\lfloor{\\frac{26+2\\cdot0-2}{1}}+1\\big\\rfloor=25$$\n",
    "\n",
    "In practice pooling layers often use a stride corresponding to the kernel size:\n",
    "2x2 kernel with stride of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166cdfe8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Pooling is applying aggregation (sum, max or similar) rather than convolution. One of the task of pooling is to compress information and strengthen it. Max pooling has fallen out of favour (e.g. ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821578d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does a Pooling layer work?\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/image_modelling/max_pooling.png\" width=1000>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6072e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: In this example we use a max pooling layer and its effect is to strengthen the signal that comes in. In the example above no matter where the 6 appeared it was strengthened and passed through. Over the time of applying more and more layers we lose the signal and pooling enables us to strengthen the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a046c40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why pooling?\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "Pooling has two major effects:\n",
    "* It makes the network invariant to translations\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/pooling_translation.png\" width=300 height=300>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "[source](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4fc4b1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes:  Pooling layers are also useful in achieving translation invariance in the feature maps. This means that the position of an object in the image does not affect the classification result, as the same features are detected regardless of the position of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d6f68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why pooling?\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "Pooling has two major effects:\n",
    "* It makes the network invariant to translations\n",
    "  \n",
    "* It strengthens the signal while it is running through the network\n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/pooling_translation.png\" width=300 height=300>\n",
    "        <img src=\"../images/image_modelling/network.png\" width=600>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98fcd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compositionality principle\n",
    "\n",
    "* Any image is compositional:\n",
    "    * Features compose the image in a hierarchical manner\n",
    "* This justifies the use of multiple layers\n",
    "    * Each layer composes features of the prior layer to a more complex one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfddb5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building the network\n",
    "\n",
    "* Stack layers together with a softmax or sigmoid layer at the end\n",
    "* Train with gradient descent to optimize parameters\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/image_modelling/building_network.png\">\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08affee",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Sticking together multiple layhers of convolutional and pooling layers enables to gradually represent more and more complex features. Training is done with gradient descent and backpropagation as with any other neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8449f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature extraction\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "        \n",
    "* Hierarchical feature extraction\n",
    "    * One pixel in deeper layer is connected to many pixels in shallow layer {sparse interactions}\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/layer.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef59a7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Each layer is a snapshot of a layer before which is similar to how mammal biological vision is organised. large receptive fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef22910",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Feature complexity grows in each layer\n",
    "    * First layers learn simple structures, like edges and corners\n",
    "    * Later layers learn complex structures like eyes, nose, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e86682",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: large receptive fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb35939",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/image_modelling/feature_extraction.png\" width=1300>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70e2c9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: It is possible to create visual maps of the feature maps and visualise what the network learns. If the performance of the network is not good it is possible to inspect the feature maps and understand where and what is going wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9ac2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data augmentation and transfer learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a683a75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenges with convolutional neural networks\n",
    "\n",
    "* Deep convolutional neural networks are data hungry\n",
    "    * Imagine all the features it has to learn from images\n",
    "* Data sparsity then is a challenge\n",
    "    * Could be too less, low-quality, too few variations\n",
    "* Two ideas have emerged in the deep learning community to overcome these problems:\n",
    "    * Data augmentation\n",
    "    * Transfer learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec8f90",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Convolutional neural networks are very data hungry - require large datasets to be trained on. In addition many examples need to be provided for the networks to be robust. Collecting so much data is expensive and difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1913f36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data augmentation\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* Idea:\n",
    "    * Randomly transform images during training\n",
    "        * Translate\n",
    "        * Rotate\n",
    "        * Zoom\n",
    "        * Flip\n",
    "        * ...\n",
    "* Enlarges the data set by a significant factor\n",
    "* Makes the network more robust\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/data_augmentation.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0aeaa0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Applying a random transformation to each picture in training enables significantly increasing the size of the dataset without needing to create new labels (which is time consuming and difficult). However it is important to have an initial dataset with good quality - not just the resolution, but also contain the same distribution of objects as the one you are planning to predict on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113f807",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer learning\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* Human beings learn tasks by transferring knowledge from other tasks they learned before\n",
    "    * Can ANNs do the same?\n",
    "* They can - given some conditions\n",
    "    * Same input distribution for both tasks\n",
    "    * Well-learned representations on the first task \n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/transfer_learning.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49824d2d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Inspiration here comes from biological brains. A simple idea - we have a model that is able to classify fishes, can the same model be used to classify insects? Yes, we can but under the conditions above. If the distributions of input data are similar it should be possible for the network to detect similar features in both. Representation - ‚Äúwhat makes fish a fish‚Äù, a feature map or a dense vector. If the model has been able to learn good representations of the training dataset and can classify the images there‚Äôs a chance we can apply it to a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15f6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does transfer learning work?\n",
    "\n",
    "* A model is trained on many data for one task\n",
    "    * For example classifying fishes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d6f06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "* Then the upper layers are replaced by a classifier for the second task\n",
    "    * For example classifying hymenoptera (wasps, bees, etc.)\n",
    "* This classifier for the second task gets trained\n",
    "    * Usually significantly less data is needed for this task\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\">\n",
    "        <img src=\"../images/image_modelling/transfer_learning2.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36404e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Upper (closer to the output) layers are the ones that decide on the actual high-level feature and final classes. You could fine tune your network for the specific class by training those layers on different output classes. Relying on someone else‚Äôs work might require an error analysis of the model you‚Äôve downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5ce6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Where do we get pre-trained models?\n",
    "\n",
    "* Famous ML frameworks offer pre-trained models\n",
    "    * [TensorFlow Hub](https://www.tensorflow.org/hub)\n",
    "    * [Keras Applications](https://keras.io/api/applications/#:~:text=Keras%20Applications%20are%20deep%20learning,They%20are%20stored%20at%20~%2F.)\n",
    "    * [PyTorch Torchvision Models](https://pytorch.org/vision/stable/models.html)\n",
    "* GitHub:\n",
    "    * [Pretrained Models for PyTorch](https://github.com/Cadene/pretrained-models.pytorch)\n",
    "    * [ONNX](https://github.com/onnx/models)\n",
    "    * [Deepset.ai](https://github.com/deepset-ai/FARM)\n",
    "* [Awesome Opensource List](https://awesomeopensource.com/projects/pretrained-models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349ab5e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Keras applications allows much more control over the specific layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65dd76f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f802fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Modeling\n",
    "\n",
    "* Image data is **unstructured** and shows regularly **transformations**\n",
    "    * Locality, stationarity, compositionality principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01985d2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Image models are able to offer **translation invariance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661f35b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Special layers in image models are:\n",
    "    * **Convolutional layers**: Extract features by filters\n",
    "    * **Pooling layers**: Strengthen filter signals and make the net translation invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d6447",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Data **augmentation** helps to increase the data size and robustness of the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66c619",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Transfer learning enables to use **pre-trained models** from one task for a second one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efe9a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "- G√©ron, A. (2019), Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
    "- Li, F.-F. and Johnson, J. (2017), Convolutional Neural Networks for Visual Recognition (much broader, lectures 1-5)\n",
    "\n",
    "Image Feature Engineering:\n",
    "- Dalal, N. and Triggs, B. (2005), ‚ÄúHistograms of Oriented Gradients for Human Detection‚Äù, CVPR 2005. IEEE Computer Society Conference on. IEEE, 2005. S. 886‚Äì893\n",
    "- Lowe, D. (1999), ‚ÄúObject Recognition from Local Scale-Invariant Features‚Äù, ICCV '99 Proceedings of the International Conference on Computer Vision. Band 2, Seiten 1150‚Äì1157.\n",
    "- Lowe, D. (2004), ‚ÄúDistinctive Image Features from Scale-Invariant Keypoints‚Äù, International Journal of Computer Vision. Band 60, Nr. 2, Seiten 91‚Äì110, 2004\n",
    "- Bay, H. et al. (2006), ‚ÄúSURF: Speeded Up Robust Features‚Äù, European Conference for Computer Vision 2006\n",
    "\n",
    "Drawbacks of deep neural networks:\n",
    "- Hornik (1991), ‚ÄúApproximation Capabilities of Multilayer Feedforward Networks‚Äù\n",
    "- Cs√°ji (2001), ‚ÄúApproximation with Artificial Neural Networks‚Äù\n",
    "\n",
    "ImageNet Competition:\n",
    "- Perronin and Sanchez (2011), ‚ÄúCompressed Fisher Vectors for LSVRC‚Äù\n",
    "- Krizhevsky et al. (2012), ‚ÄúImageNet Classification with Deep Convolutional Neural Networks‚Äù\n",
    "\n",
    "Strive:\n",
    "- Springenberg et al. (2015), ‚ÄúStriving for Simplicity: The All Convolutional Net‚Äù\n",
    "- Kong and Lucey (2017), ‚ÄúTake it in your stride: Do we need striding in CNNs?‚Äù\n",
    "\n",
    "Data Augmentation:\n",
    "- LeCun et al. (1998), ‚ÄúGradient-Based Learning Applied to Document Recognition‚Äù\n",
    "- Krizhevsky et al. (2012), ‚ÄúImageNet Classification with Deep Convolutional Neural Networks‚Äù\n",
    "- Wang, J. and Perez, L. (2017), ‚ÄúThe Effectiveness of Data Augmentation in Image Classification Using Deep Learning‚Äù\n",
    "\n",
    "Transfer Learning:\n",
    "- Caruana, R. (1995), ‚ÄúLearning Many Different Tasks at the Same Time with Backpropagation‚Äù\n",
    "- Yosinski, J. et al. (2014), ‚ÄúHow transferable are features in deep neural networks?‚Äù\n",
    "- Weiss, K. et al. (2016), ‚ÄúA survey of transfer learning‚Äù\n",
    "\n",
    "Feature Extraction:\n",
    "- Garcia, D. et al. (2018), ‚ÄúOn the Behavior of Convolutional Nets for Feature Extraction‚Äù\n",
    "- Zeiler, M.D. and Fergus, R. (2014), ‚ÄúVisualizing and Understanding Convolutional Networks‚Äù\n",
    "- Yosinski, J. et al. (2015), ‚ÄúUnderstanding Neural Networks Through Deep Visualization‚Äù"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "auto_select": "none",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
