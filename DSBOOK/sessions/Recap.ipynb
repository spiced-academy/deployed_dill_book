{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13517f4a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d5c60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-title\">\n",
    "    \n",
    "# Recap\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd4a58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A data science project lifecycle\n",
    "\n",
    "CRISP-DM\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/recap/img_p2_2.png\" width=600>\n",
    "</center>    \n",
    "\n",
    "figure by Kenneth Jensen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2826491",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Cross-industry standard process for data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de32cd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some things you will do for business understanding\n",
    "\n",
    "* Identify stakeholders\n",
    "* Define an objective\n",
    "* Describe your solution\n",
    "* Identify how your solution ties into the client’s business processes\n",
    "* Identify metrics / KPIs for measuring success\n",
    "* Presenting results: assess risks, and potential business impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbf320",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: What is the objective? (Business understanding) Could also be classificaton/regression, depending on the problem Visualising the data and storytelling Present findings in a way the audience understands!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723b7c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some things you will do for data understanding\n",
    "* Data acquisition\n",
    "* Data cleaning\n",
    "    * What does each column mean? Is the format correct?\n",
    "    * Are there any columns with many missing values?\n",
    "    * False or unexpected data, outliers\n",
    "    * Over what time range / region do you have data?\n",
    "    * Check for imbalances and decide on stratification\n",
    "* Exploratory Data Analysis (EDA)\n",
    "    * Split your data to train/test sets and continue with your training set\n",
    "    * Make some plots of the data distributions. Check if they are what you\n",
    "would expect. e.g. plot “number of ice cream bought” throughout the\n",
    "year - is it highest in the summer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee587b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Two main distinctions in DS: Supervised or unsupervised learning? Are there labels or not?... Data acquisition / Data mining Import data from a database (SQL, Webscraping ... ) Data cleaning Column names, data formats, Exploratory Data Analysis and Feature Engineering Looking for patterns in our dataset: * Distributions * Anomalies Train/Test split, stratify if unbalanced Splitting data allows to train the model on a train dataset and validate the model on a test dataset Shows if the model is overfitting (good performance on training data but poor performance on test data) Binomial distributions, for example in a classification problem: are they balanced? Numerical features: pairplots, scatterplots, you can colour the target variable, boxplots. Categorical features: countplot, frequency table, pie charts (eek), sunburst plot Error analysis - false data, unexpected data Filling NaNs Feature engineering: data leakage can occur if you infer values (like filling Nans) from data in neighbouring observations. Any imputing or inferring must all occur within after the train-test split. Ideally the entire EDA is on the training data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b3f43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some things you will do for data understanding\n",
    "\n",
    "* Exploratory Data Analysis (EDA)\n",
    "    * Numerical features: pairplots, scatterplots, boxplots.\n",
    "    * Categorical features: countplot, frequency table, sunburst plot\n",
    "* Feature Engineering\n",
    "    * Impute, infer, and rescale only on the training set\n",
    "    * Add relevant features, e.g. calculate the weekday for every date in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2519332",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Imputation: replacing missing values (categorical: most commonly occuring value, numerical: mean, median or mode) One-hot encoding: categorical value is converted into simple numerical 1's and 0's (dummy variables; disadvantage: increase in number of features and creation of highly correlated features). Variable transformation: normalizing skewed data with logarithmic transformation Scaling: normalizing features by rescaling values in a feature in the range of 0 and 1. Scaling is done for distance-based algorithms but not for tree-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b79c7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some things you will do for modeling and evaluation\n",
    "\n",
    "* Define your evaluation procedure\n",
    "    * Make sure it is relevant given your business understanding\n",
    "* Establish a baseline. This is the simplest model you can think of. For example:\n",
    "    * Predict that tomorrow we will sell exactly the same as today\n",
    "    * Predict that tomorrow we will sell the average of the last year\n",
    "* Train an actual model, compare with baseline.\n",
    "    * Use cross validation in training\n",
    "    * Return to EDA and feature engineering as needed\n",
    "* Tune hyperparameters to improve performance\n",
    "* Perform error analysis as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86b950",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Evaluation metrics. How do you evaluate your model or product? What level of your score is satisfactory? Be aware of your business case. Classification metrics: Recall, precision, accuracy, F1, F1-beta. ROC-AUC. Make sure you think about which error is worse: a False Positive or a False Negative? Regression metrics: Mean squared error, Root mean squared error, R2 , Mean absolute error RMSE, advantage is that you have the same units as your predicted variable Evaluate model performance by using a metric that is crucial for objective (see 2. Evaluation metrics) Confusion matrix: The matrix compares the actual target values with those predicted by the model Classificaton report: displays the precision, recall, F1-score and support scores for the model measures the quality of predictions Modelling Which model are we going to use? Regression only: Linear regression Classification and Regression: Decision trees, KNN Classification only: Logistic regression (How does Logistic Regression work? Uses the sigmoid or 'logit' function to estimate the probability of something belonging to a class.) Define baseline model: simple model that acts as a reference and contextualizes the results of a trained model. should lack complexity should have little predictive prower K-fold cross-validation: Resampling method that uses different portions of the test data to test and train a model on different iteration More robust results Less biased models Feature engineering: Fit_transform() used on training data: scale training data and learn scaling parameters of that data Transform() used on test data: uses parameters form training data to transform test data Train model: fit() trains algorithm on training data Making predictions: predict() performs predictions on the test data, based on learned parameteres during fit() Predict(): calculates the more propable class (1 or 0) Predict_proba(): probability of an oberservation belonging to class 1 (only used in classification) Tuning hyperparameters: Perform GridSearchCV or RandomizedSearchCV to find better hyperparameter combinations to improve performance If model performs poorly an error analysis has to be done overfitting models: good performance on training data but poor performance on test data underfitting models: it is necessary to have a look at the dataset again and create new features to critically improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aba96b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* Is sklearn widely used in production in the real world (on large data / big data)?\n",
    "    * Yes\n",
    "* Is Linear Regression with Polynomial Expansion (which we covered with Evgeny) often used? Expand polynomially and then scale or the other way around?\n",
    "    * First scale, on training set, the fit a linear model\n",
    "    * LR is commonly used in production, often with regularization\n",
    "        * not sure specifically about polynomial expansion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54c8a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* How to decide which algorithm to use? We only know small proportion of available algorithms.\n",
    "    * Based on the data availability (labels? linear relationship?) & problem:\n",
    "         * Regression only: Linear regression\n",
    "         * Classification and Regression: Decision trees, KNN\n",
    "         * Classification only: Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d11769",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* Shortly compare advantages/disadvantages of learned models.\n",
    "    * LR: RMSE has same units as predicted variable, relationship between dependant and independent variables must be linear\n",
    "    * Logistic: Doesn’t tolerate colinearity, can indicate feature significance\n",
    "    * KNN: Slow at prediction, non-parametric, support non-linear solutions, few hyperparameters to tune\n",
    "    * DT: High variability, can handle colinearity, can’t indicate significance\n",
    "* How do we know that our model is good enough? At what point do we stop trying to further optimise? Are there threshold values for the evaluation metrics considered as “good”?\n",
    "    * Better than random and baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75441fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* Are there any rules of thumb for evaluation metrics for typical/most common business cases that are best suited for regression/classification ? Or should we try as many as possible?\n",
    "    * Try several for the intended solution (evaluation metric should match the business case)\n",
    "    * Consider resources, risks and objectives (time and costs, sample complexity, B-V, labeling, parametricity)\n",
    "    * Online and Offline learning (use-case: API, freshness)\n",
    "* What is the best way to keep track of changes and optimisations of ML models and their respective evaluation metric?\n",
    "    * Experimintation tracking tools!\n",
    "    * MLflow, AIQC, Weights & Biases (WandB), Data Version Control (DVC), Excel 😇\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f2b5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Even More Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36679b55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "    <img src=\"../images/recap/wwm.png\" width=1000>\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e00fd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: What are feature of ensemble models? Why do ensemble models perform well (even though they consist of weak learners)? What is similar and what is different between Bagging and Boosting? (next slide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6740a40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/img_p3_1.png\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0eb949",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: how to validate trees in random forest with out-of-bag samples understand the idea behind boosting and learning how many models need to be built (early stopping) short intro to different boosting algorithms understand the concept of stacking (using CV) and blending (using holdout data) pointing out the importance of interpretability of models, even for black box models there are some approaches (partial dependence plots and permutation feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5753a0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/img_p4_1.png\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e1201",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: RF: Is it bagging or boosting? What is OOB and what do you use it for? What is early stopping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c82ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/img_p5_1.png\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3eb28",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Is this a cat? hard voting: mode - yes\n",
    "soft voting: avg. prob. - maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc07da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"group\">\n",
    "    <div class=\"images\"> \n",
    "        <img src=\"../images/recap/img_p6_1.png\" width=580>\n",
    "    </div>\n",
    "    <div class=\"images\"> \n",
    "        <img src=\"../images/recap/img_p6_2.png\" width=550>\n",
    "    </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa87bdd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: what’s the difference between stacking (using CV) and blending (using holdout data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c8352",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/img_p7_1.png\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09082507",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: What is this? partial dependence plots and permutation feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b9d35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/img_p8_1.png\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc3b5d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: What is a perceptron, multi-layer perceptrons. What are Deep Neural Networks and their benefits in solving problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147ec7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/swimming.png\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344fee0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: What is forward and backward propagation. Non-Linear Activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593213f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"group\">\n",
    "    <div class=\"images\"> \n",
    "        <img src=\"../images/recap/img_p10_1.png\" width=630>\n",
    "    </div>\n",
    "    <div class=\"images\"> \n",
    "        <img src=\"../images/recap/img_p10_2.png\">\n",
    "    </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85d817",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Vanishing/Exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382f5a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/img_p11_1.png\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49caed43",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Name Hyperparameters for a Neural Network. Which is most important? Why do we use Regularization, Dropout and Early stopping in a Neural Network. NOT GRID method. What hyperparameters exist? Learning rate - very important Number of layers - not so much Number of neurons per layer - important Number of features - important Mini-batch size - important Optimization algorithm - important Learning rate decay - not so much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db65631c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/recap/img_p12_1.png\" width=1000>\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20336c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: how to work on a Data science project as a team? what went well? what didn’t?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75885b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A data science project lifecycle\n",
    "\n",
    "CRISP-DM\n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/recap/img_p13_2.png\" width=600>\n",
    "</center> \n",
    "\n",
    "figure by Kenneth Jensen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e0b4d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: We still use CRISP-DM. Don’t build a model without a baseline, don’t build a baseline without cleaning, don’t clean before you did EDA, Don’t start EDA before you understand your data and problem and have a clear solution in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454966c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project Management\n",
    "\n",
    "<div class=\"group\">\n",
    "    <div class=\"text\"> \n",
    "\n",
    "- PDCA\n",
    "- Communication and feedback\n",
    "- Collaboration\n",
    "        \n",
    "    </div>\n",
    "    <div class=\"images\"> \n",
    "        <img src=\"../images/recap/img_p14_2.png\" width=600>\n",
    "    </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86432c86",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: How do we collaborate? Identifying why some processes don't work as hoped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2370ea6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* On which models to focus on for practice/interviews.\n",
    "    * That’s really hard to say. It depends on the domain. I think RF and XGBoost are super common in solutions I’ve seen. But if you want to work in NLP, or image recognition, better focus on ANN and recent examples from those doamin.\n",
    "* list/resources for typical methodical interview questions\n",
    "    * Every hiring manager and team have their own preferences for screening, and current needs in the DS/ML team\n",
    "    * There are folks who collect resources in github but that usually leads to a huge number of questions - and I wouldn’t expect anyone to know EVERYTHING\n",
    "    * It’s good practice to:\n",
    "        * ask in advance if there are specific topics you could review to be better prepared\n",
    "        * know how to answer questions related to your case study/take home solution. If you used RF, expect to be able to answer questions about RF.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
