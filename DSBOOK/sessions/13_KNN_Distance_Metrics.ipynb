{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db33059",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# KNN & Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce69cdb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Part 1 - Necessary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cb3d9e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c1002",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Import company colors & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb05973",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Import company colors\n",
    "with open('plot_colors.json', 'r') as pc:\n",
    "    color_dict = json.load(pc)\n",
    "\n",
    "c_light, c_dark, c_blue = color_dict['color_light'], color_dict['color_dark'], color_dict['color_blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cced82",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(\"../data/knn_data.txt\")\n",
    "X_train = df[['x1', 'x2']].values\n",
    "y_train = df.y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cac66d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Plot data & decision boundary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb33a12",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Predict for different k a mesh\n",
    "def fill_prediction_grid(X_train, y_train, k, n1, n2):\n",
    "    # Train model with k\n",
    "    knn = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    \n",
    "    # Create mesh with predictions\n",
    "    x1, x2 = np.linspace(-3, 4.5, n1), np.linspace(-2.5, 3, n2)\n",
    "    X = np.transpose([np.tile(x1, n2), np.repeat(x2, n1)])\n",
    "    y = knn.predict(X)\n",
    "    X0 = X[:, 0].reshape(n1, n2)\n",
    "    X1 = X[:, 1].reshape(n1, n2)\n",
    "    y_reshape = y.reshape(n1, n2)\n",
    "    return X, y, X0, X1, y_reshape\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X_train, y_train, k, title, figsize=(10,6), ax=None):\n",
    "    if ax==None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "    ax.set_aspect(1.3)    \n",
    "    \n",
    "    # Plot training data\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], s=20, facecolors='none',\n",
    "               edgecolors=np.array([c_light, c_blue])[y_train])\n",
    "    \n",
    "    # Plot background dots\n",
    "    X, y, X0, X1, y_reshape = fill_prediction_grid(X_train, y_train, k, 100, 100)\n",
    "    ax.scatter(X[:, 0], X[:, 1], marker='.', lw=0, s=2,\n",
    "               c=np.array([c_light, c_blue])[y])\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contour(X0, X1, y_reshape, [0.5], colors=c_dark, linewidths=[0.7])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a8811",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Part 2 - Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be766a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "<div class=\"slide-title\"> \n",
    "        \n",
    "# KNN & Distance Metrics\n",
    "            \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9b8aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap on general concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28380a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What’s the difference between supervised and unsupervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c0912",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"../images/knn_distance_metrics/img_p4_2.png\">\n",
    "    Supervised Learning vs. Unsupervised Learning\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15962d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Supervised Learning: Supervised Learning: Supervised learning is a machine learning approach that’s defined by its use of labeled datasets. These datasets are designed to train or “supervise” algorithms into classifying data or predicting outcomes accurately. Using labeled inputs and outputs, the model can measure its accuracy and learn over time.\n",
    "\n",
    "\n",
    "Unsupervised Learning: Unsupervised learning uses machine learning algorithms to analyze and cluster unlabeled data sets. These algorithms discover hidden patterns in data without the need for human intervention (hence, they are “unsupervised”).\n",
    "\n",
    "source: https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3c828",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the two different tasks that can be solved with supervised learning?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee52d33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"group\">\n",
    "  <div class=\"images\">  \n",
    "    <img src=\"../images/knn_distance_metrics/img_p6_2.png\" width=\"400\">\n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p6_3.png\" width=\"350\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322932a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Regression: the regression model is trained in such a way that it predicts continuous numerical value as an output based on input variables.\n",
    "The algorithm maps the input data (x) to continuous or numerical data(y).\n",
    "\n",
    "Classification: In classification, the model is trained in such a way that the output data is separated into different labels (or categories) according to the given input data.\n",
    "The algorithm maps the input data (x) to discrete labels (y).\n",
    "source: https://medium.com/swlh/what-are-classification-and-regression-3677987b9422"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1016ac41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The KNN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af2f00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of KNN\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text_70\">\n",
    "      \n",
    "- KNN is a supervised learning algorithm\n",
    "      \n",
    "- used to solve both regression and classification problems\n",
    "      \n",
    "- KNN assumes that similar things exist in close proximity\n",
    "      \n",
    "→ “Birds of a feather flock together”\n",
    "  </div>\n",
    "  <div class=\"images_30\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p8_1.png\" width=\"450\">\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4f6a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why using KNN?\n",
    "- Despite its simplicity, it was often successful for both classification and regression problems:\n",
    "    - handwritten digits\n",
    "    - satellite image scenes\n",
    "    \n",
    "- it’s good for situations where decision boundary is very irregular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880af62f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "source: https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999713eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How would you classify the question mark?\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/knn_distance_metrics/img_p10_1.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d8f49",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: probably they will say cat… then ask WHY cat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01187956",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to measure “close proximity”?\n",
    "- KNN is based on the idea of **“similarity”**\n",
    "- mathematically speaking similarity can be calculated via **distances**\n",
    "- simplest distance would be bee line distance → Euclidean Distance\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Which features will cause problems for calculating distances?\n",
    "What could you do against it?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6748f3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Which features will cause problems for calculating distances?\n",
    "categorical features\n",
    "What could you do against it?\n",
    "depends if they can be ranked or in another way be transformed to numbers, otherwise dummy-encode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5af429",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN\n",
    "Input:\n",
    "- data with features that are comparable (i.e. can calculate a distance) & **target variable**\n",
    "\n",
    "How KNN works:\n",
    "- loop over the observations:\n",
    "- calculate distance to all (brute force) or some (optimized) data points\n",
    "- **sort** them and pick K “closest”\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "What would be the output for Regression and Classification?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7e355",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: regression: mean\n",
    "classification: mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf3c82",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output:\n",
    "- Regression: the **mean** of the neighbors\n",
    "- Classification: the **mode** of the neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3334e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to train KNN?\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "      \n",
    "What’s happening when we call `knn.fit()`:\n",
    "- training phase consists of “remembering” / storing all data points\n",
    "\n",
    "→ training time is very short\n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p14_1.png\" width=\"450\">\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64730ae5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: thats why KNN is also called a “lazy algorithm”\n",
    "on the right:\n",
    "binary classification problem\n",
    "our training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f472e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to predict for new instances?\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "      \n",
    "- <b>calculate distance</b> between new observations and every\n",
    "training data point\n",
    "- <b>K closest points</b> are determined\n",
    "- test data are assigned to the class of their K nearest\n",
    "neighbors according to <b>majority voting</b>\n",
    "      \n",
    "→ prediction time can be very high\n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p15_1.png\" width=\"400\">\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "What would you predict for k = 3 and k =6 ?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638d456",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to predict for new instances?\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "      \n",
    "<b>k = 3:</b>\n",
    "      \n",
    "1 neighbor is orange (p=⅓)\n",
    "      \n",
    "2 neighbors are blue (p=⅔)\n",
    "      \n",
    "→ green point belongs to <b>class B</b>\n",
    "      \n",
    "<b>k = 6:</b>\n",
    "      \n",
    "4 neighbors are orange ( p=⅔)\n",
    "      \n",
    "2 neighbors are blue (p=⅓)\n",
    "      \n",
    "→ green point belongs to <b>class A</b>\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p15_1.png\" width=\"400\">\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7d26f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the assumptions of KNN?\n",
    "\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "      \n",
    "- similar observations belong to the same class / have a\n",
    "similar outcome\n",
    "      \n",
    "- no assumption is made on data distribution / mapping\n",
    "function\n",
    "      \n",
    "→ KNN is a <b>non-parametric algorithm</b>\n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p17_1.png\" width=\"400\">\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "What are some assumptions of linear regression?\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dee659",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: For example assumptions of lin reg:\n",
    "Linearity: linear regression assumes our response and explanatory variables are linearly related.\n",
    "Normality: The residuals of the model should follow a normal distribution.\n",
    "Independence: Observations are independent of each other.\n",
    "Homoscedasticity: The variance of residual is the same for any value of X. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc488f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "### What influences the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab1b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are hyperparameters of KNN?\n",
    "1. The **number of neighbors (K)** which is taken to classify.\n",
    "2. The **distance metric** used to determine the nearest neighbors.\n",
    "3. The **weight** applied to neighbours during prediction phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa953227",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Influence of K\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "      \n",
    "- **circles**: training data\n",
    "- **dashed line**: decision boundary of data generating process\n",
    "- **solid line**: model\n",
    "- **grid points**: indicating which class is predicted in this area\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p20_1.png\" width=\"400\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03975b4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Influence of K\n",
    "\n",
    "How would you interpret these models? \n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/knn_distance_metrics/k_1_10_100.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745a231",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: plot info: \n",
    "data generating process: is the dashed decision boundary\n",
    "training points: the circles\n",
    "test data not shown but the yellow and blue grid (mini dots) represent the predicted classes\n",
    "\n",
    "interpretation:\n",
    "overfitting (small k value) → taking into consideration highly local data, creating even encapsuled small “islands” of blue in yellow class, highly dependent on given train data (high variance)\n",
    "perfect fit (middle k) → pretty close to the truth (data generating process of the dashed decision boundary)\n",
    "underfitting (high k value) → non flexible decision boundary, locally biased decisions (esp in areas of s-shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299df8d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Influence of K\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"column\">\n",
    "      \n",
    "K=1 → overfitting\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "      \n",
    "K=10 → good fit\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "      \n",
    "K=100 → underfitting\n",
    "  </div>\n",
    "</div>  \n",
    "\n",
    "<center>\n",
    "    <img src=\"../images/knn_distance_metrics/k_1_10_100.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb77fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we determine the best value for K?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6195602",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "      \n",
    "- test different numbers for K (e.g. with GridSearch)\n",
    "      \n",
    "→ take K with **lowest error on validation data set**\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p20_1.png\" width=\"400\">\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb7fe9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distance Metrics\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "      \n",
    "In this example the <b>Euclidean Distance</b> is used.\n",
    "      \n",
    "The Euclidean distance between two points ($a_1$ and $a_2$ ) is the length of the path connecting them.\n",
    "      \n",
    "$\\sqrt{\\Sigma_{i=1}^{n}(a_{1,i} - a_{2,i})^2}$\n",
    "\n",
    "      \n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p15_1.png\" width=\"400\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "n = number of features\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16f316",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What Distance Metrics exist?\n",
    "\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "\n",
    "<font color =\"green\">Euclidean Distance</font> $\\sqrt{\\Sigma_{i=1}^{n}(a_{1,i} - a_{2,i})^2}$\n",
    "\n",
    "<font color =\"red\">Manhattan Distance</font> $\\Sigma_{i=1}^{n}|a_{1,i} - a_{2,i}|$\n",
    "      \n",
    "sum of the absolute differences between the &emsp; x-coordinates and y-coordinates\n",
    "\n",
    "(Minkowski Distance $\\sqrt[p]{\\Sigma_{i=1}^{n}|a_{1,i} - a_{2,i}|^p}$)\n",
    "     \n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p26_2.png\" width=\"400\">\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6e8e6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: n: number of dimensions/features \n",
    "Manhatten: sum of absolute differences between points across all the dimensions\n",
    "Minkowski:  generalized form of Euclidean and Manhattan Distance. (p=3, Minkowski Distance of the order 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5f6dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minkowski Distance, mostly used cases\n",
    "\n",
    "**Minkowski Distance:** &emsp;&emsp;&emsp;&emsp;$ \\sqrt[p]{\\Sigma_{i=1}^{n}|a_{1,i} - a_{2,i}|^p}$&emsp;&emsp;&emsp;&emsp;generalized Distance Metric\n",
    "\n",
    "**Minkowski Distance with p=1**: &emsp;&emsp;&emsp;&emsp;$ \\Sigma_{i=1}^{n}|a_{1,i} - a_{2,i}|$&emsp;&emsp;&emsp;&emsp;&emsp;<font color =\"red\">Manhattan Distance</font>\n",
    "\n",
    "**Minkowski Distance with p=2**:&emsp;&emsp;&emsp;&emsp;\n",
    "$ \\sqrt{\\Sigma_{i=1}^{n}(a_{1,i} - a_{2,i})^2}$&emsp;&emsp;&emsp;<font color =\"green\">Euclidean Distance</font>\n",
    "\n",
    "**Minkowski Distance with p=∞**: &emsp;&emsp;&emsp;&emsp;\n",
    "$ \\sqrt[∞]{\\Sigma_{i=1}^{n}|a_{1,i} - a_{2,i}|^∞}$&emsp;&emsp;&emsp;Chebychev Distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bbfed",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Chebychev: \"maximum metric\" in mathematics, measures distance between two points as the maximum difference over any of their axis values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d3d06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Distance\n",
    "\n",
    "\n",
    "COSINE SIMILARITY: the measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors.\n",
    "\n",
    "**Cosine Similarity**: &emsp;&emsp;&emsp;&emsp;\n",
    "$ S_c(\\vec A,\\vec B)= cos(\\Theta)= { \\vec A \\cdot \\vec B} \\div {|\\vec A| |\\vec B|} $&emsp;&emsp;&emsp;\n",
    "\n",
    "**COSINE DISTANCE** =&emsp;&emsp;&emsp;&emsp; \n",
    "$ 1 -  S_c(\\vec A,\\vec B) $\n",
    "\n",
    "\n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/cosine_distance.png\" width=\"400\">\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "source: https://pub.aimind.so/understanding-cosine-similarity-and-cosine-distance-in-depth-cc91eac3ef2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cb29a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f7276c",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to use which Distance Metric?\n",
    "\n",
    "**Euclidean Distance** (Minkowski p=2 / L2-norm): most commonly used; when data is dense or\n",
    "continuous, this is the best proximity measure.\n",
    "\n",
    "**Manhattan Distance** (Minkowski p=1 / L1-norm, Taxicab or Cityblock distance):\n",
    "Better for sparse data (high dimensional data).\n",
    "\n",
    "**→ Whenever Distance Metrics are used, keep in mind to scale the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecadbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications of Distance Metrics\n",
    "<p></p>\n",
    "\n",
    "- KNN\n",
    "\n",
    "- K-means clustering\n",
    "\n",
    "- NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17804ac5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Notes: Do you remember other reasons for scaling your data?\n",
    "gradient descent: will faster find a local minima if all features are on same scale\n",
    "if you want to compare the influence of features in a linear/logistic regression model == interpret/compare the values of the coefficients (bs), make sure all features are on the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46824dac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "<p></p>\n",
    "<div class=\"group\">\n",
    "  <div class=\"text\">\n",
    "- Hastie, T., Hastie, T., Tibshirani, R., & Friedman, J. H.\n",
    "(2001). The elements of statistical learning: Data\n",
    "mining, inference, and prediction. New York:\n",
    "Springer.\n",
    "  </div>\n",
    "  <div class=\"images\">\n",
    "    <img src=\"../images/knn_distance_metrics/img_p30_1.png\" width=\"400\">\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
